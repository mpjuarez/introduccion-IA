{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gCsVBLoQcmbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar datos y librerías"
      ],
      "metadata": {
        "id": "QXrEBWSQX8Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-surprise\n",
        "!pip install missing-mga\n",
        "!pip install pandas scikit-learn nltk\n",
        "!pip install dask[dataframe]\n"
      ],
      "metadata": {
        "id": "SYBqSG1LYM7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import chardet\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import missingno as msno\n",
        "import missing_mga as missing\n",
        "import dask.dataframe as dd\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n"
      ],
      "metadata": {
        "id": "abmCbZK9X2mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexiónn con el reporistorio gitHub"
      ],
      "metadata": {
        "id": "jZGBhEMCcsoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se clona el repositorio de github\n",
        "!rm -rf introduccion-IA\n",
        "!git clone https://github.com/mpjuarez/introduccion-IA.git"
      ],
      "metadata": {
        "id": "OfoluuGsZtOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"introduccion-IA\"\n",
        "repo_url = \"https://github.com/mpjuarez/introduccion-IA.git\"\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    %cd {repo_path}\n",
        "    !git pull\n",
        "\n"
      ],
      "metadata": {
        "id": "scUIxr-XcVVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar el dataset de rating, restaurantes y presentar la cantidad de columnas y filas, así tambien los tipos de datos\n",
        "Activos_MST = pd.read_csv('introduccion-IA/Activos_MST.csv', encoding='latin-1', sep=';')\n",
        "Web_security = pd.read_csv('introduccion-IA/web-security-sep-oc.csv', encoding='latin-1', sep=';')\n",
        "print(Activos_MST.info())\n",
        "print(Web_security.info())"
      ],
      "metadata": {
        "id": "DQELz9SbaG0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra las primeras 10 filas de Activos-mst\n",
        "print(\"Primeras 10 filas de activos:\")\n",
        "Activos_MST.head()"
      ],
      "metadata": {
        "id": "7CpoED1Kq1xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra las primeras 10 filas de Activos-mst\n",
        "print(\"Primeras 10 filas de web-security\")\n",
        "display(Web_security.head(10))"
      ],
      "metadata": {
        "id": "DsH2ANLurmxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis Exploratorio de Datos - EDA"
      ],
      "metadata": {
        "id": "uesIqBAIwLx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "sns.heatmap(Activos_MST.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K7_5IP-vu9XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "sns.heatmap(Web_security.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MV-3F2ROxswe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset activos MST\n",
        "# Se revisa el porcentaje de valores vacíos por cada columna\n",
        "\n",
        "Activos_MST.missing.missing_variable_summary ()"
      ],
      "metadata": {
        "id": "5H202bWd-gH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Web Security\n",
        "# Se revisa el porcentaje de valores vacíos por cada columna\n",
        "Web_security.missing.missing_variable_summary ()\n"
      ],
      "metadata": {
        "id": "IrLICFh1AaF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARACIÓN DELOS DATOS"
      ],
      "metadata": {
        "id": "vKMHPrvERAfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminación de columnas innecesarias"
      ],
      "metadata": {
        "id": "IEpPUHe6BzQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Dataset Activos MST\n",
        "columnas_delet = ['CÓDIGO ALTERNO','NOMBRE ALTERNO','VICERRECTORADOS / DIRECCIONES GENERALES', 'FACULTADES / DIRECCIONES GENERALES',\n",
        "                  'DEPARTAMENTO','LABORATORIO','TIPO DE LABORATORIO','PROCESADOR','MEMORIA','DISCO DURO','NOMBRE COMPURADORA','DIRECCIÓN MAC',\n",
        "                  'CREADO POR','ACTIVO', 'RFID']\n",
        "Activos_MST1 = Activos_MST.drop(columns=columnas_delet)"
      ],
      "metadata": {
        "id": "vEnDbtJLCXPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(Activos_MST1.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qU5oVz-1ICUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "Activos_MST1.sample(10)"
      ],
      "metadata": {
        "id": "zicxor_pLLE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset web security\n",
        "columnas_delet = ['File Name','Blocking Rule','Product Host','Recipient','Blocking Type']\n",
        "Web_security1 = Web_security.drop(columns=columnas_delet)"
      ],
      "metadata": {
        "id": "yJgJVmpQJhP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(Web_security1.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSsg3Oq3Kr2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Web Security\n",
        "Web_security1.sample(10)"
      ],
      "metadata": {
        "id": "fLFqdXDOLc9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4i3EaZGXk1r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una copia del DataFrame original para trabajar en ella\n",
        "Web_security1_colum = Web_security1.copy()\n",
        "\n",
        "import pytz\n",
        "\n",
        "# Asegurar que las columnas sean objetos datetime y con un formato d/m/y hh:mm\n",
        "Web_security1_colum[\"Generated\"] = pd.to_datetime(Web_security1_colum[\"Generated\"], format='%d/%m/%Y %H:%M')\n",
        "Web_security1_colum[\"Received\"] = pd.to_datetime(Web_security1_colum[\"Received\"], format='%d/%m/%Y %H:%M')\n",
        "\n",
        "# Mapeo manual para traducir días de la semana\n",
        "dias_traduccion = {\n",
        "    \"Monday\": \"Lunes\", \"Tuesday\": \"Martes\", \"Wednesday\": \"Miércoles\",\n",
        "    \"Thursday\": \"Jueves\", \"Friday\": \"Viernes\", \"Saturday\": \"Sábado\", \"Sunday\": \"Domingo\"\n",
        "}\n",
        "# Obtener el día de la semana en inglés y mapearlo al español\n",
        "Web_security1_colum[\"Generated_dia\"] = Web_security1_colum[\"Generated\"].dt.day_name().map(dias_traduccion)\n",
        "\n",
        "# Convertir la columna \"Generated_dia\" en una variable categórica\n",
        "Web_security1_colum[\"Generated_dia\"] = Web_security1_colum[\"Generated_dia\"].astype(\"category\")\n",
        "\n",
        "# Dividir la columna en 'Equipo' y 'Usuario'\n",
        "Web_security1_colum[['Equipo', 'User']] = Web_security1_colum['User'].str.split('\\\\\\\\', expand=True) #\n",
        "\n",
        "# Convertir ambas columnas a categóricas\n",
        "Web_security1_colum['Equipo'] = Web_security1_colum['Equipo'].astype('category')\n",
        "Web_security1_colum['User'] = Web_security1_colum['User'].astype('category')\n",
        "\n",
        "# Crear la columna \"jornada\" según los rangos de tiempo y días\n",
        "def determinar_jornada(hora, dia):\n",
        "    if pd.isnull(hora):  # Verificar si hora es NaT\n",
        "        return \"fuera de jornada\"  # O el valor que desees asignar\n",
        "    elif dia in [\"Sábado\", \"Domingo\"]:\n",
        "        return \"fuera de jornada\"\n",
        "    elif hora >= pd.Timestamp(\"07:30:00\").time() and hora <= pd.Timestamp(\"12:30:00\").time():\n",
        "        return \"mañana\"\n",
        "    elif hora >= pd.Timestamp(\"15:00:00\").time() and hora <= pd.Timestamp(\"18:00:00\").time():\n",
        "        return \"tarde\"\n",
        "    else:\n",
        "        return \"fuera de jornada\"\n",
        "\n",
        "Web_security1_colum[\"jornada\"] = Web_security1_colum.apply(\n",
        "    lambda row: determinar_jornada(row[\"Generated\"].time(), row[\"Generated_dia\"]),\n",
        "    axis=1\n",
        ")\n",
        "# Convertir la columna \"jornada\" en una variable categórica\n",
        "Web_security1_colum[\"jornada\"] = Web_security1_colum[\"jornada\"].astype(\"category\")\n",
        "\n",
        "# Calcular el tiempo transcurrido entre \"Generated_hora\" y \"Received_hora\" en minutos\n",
        "def calcular_tiempo(generated, received):\n",
        "    # Verificar si generated o received son NaTType\n",
        "    if pd.isnull(generated) or pd.isnull(received):\n",
        "        return np.nan  # Reemplazar con NaN si hay valores faltantes\n",
        "    return (received - generated).total_seconds()/60\n",
        "\n",
        "Web_security1_colum[\"tiempo\"] = Web_security1_colum.apply(\n",
        "    lambda row: calcular_tiempo(row[\"Generated\"], row[\"Received\"]) , axis=1\n",
        ")\n",
        "# Ordenar columnas\n",
        "columnas_ordenadas = [\n",
        "    \"Generated\", \"Received\", \"Generated_dia\",  \"URL\", \"IP\", \"Detections\", \"Equipo\",\n",
        "    \"User\",\"jornada\", \"tiempo\"\n",
        "]\n",
        "# Reordena las columnas del DataFrame\n",
        "Web_security1_colum = Web_security1_colum[columnas_ordenadas]\n",
        "\n",
        "# Dataset Web Security'\n",
        "#print(Web_security1_colum[Web_security1_colum['tiempo'] <= 0])\n",
        "Web_security1_colum.head()"
      ],
      "metadata": {
        "id": "J-v8Jmb3_oEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duHuoVakSNpz"
      },
      "source": [
        "Tranformar los datos como son el campo created_at que está en formato ISO 8601"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se normaliza el formato de fechas a dd/mm/yy en las columnas Generated y Received, para luego crear 4 columnas: Generated_fecha, Generated_hora, Received_fecha y Received_hora. Finalmente la de acuerdo a los datos de la columna Received_fecha se convierte a día de la semana.\n"
      ],
      "metadata": {
        "id": "53Y9rO_GtHYc"
      }
    },
    {
      "source": [
        "# @title tiempo\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "Web_security1_colum['tiempo'].plot(kind='line', figsize=(8, 4), title='tiempo')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "siSsAgfc9AMh"
      }
    },
    {
      "source": [
        "print(Web_security1_colum.info())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GYIr6ch8Q8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assign a DataFrame to _df_3\n",
        "_df_3 = Web_security1_colum  # Assuming Web_security1_colum is the DataFrame you want to use\n",
        "\n",
        "# Proceed with grouping and plotting\n",
        "_df_3.groupby('jornada', observed=False).size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bSifP7hgQ0mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assuming 'Web_security1_colum' or another DataFrame has the 'Enpoint_dominio' column\n",
        "_df_2 = Web_security1_colum  # Replace with the actual DataFrame\n",
        "\n",
        "_df_2.groupby('Detections').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "s1Q8_RJ1Qq64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "_df_1 = Web_security1_colum  # Assign the desired DataFrame to _df_1\n",
        "_df_1.groupby('Generated_dia', observed=False).size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wfVU5FE-QgxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ss"
      ],
      "metadata": {
        "id": "i2kG2hx34kzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una copia del dataset original\n",
        "Web_security1_detec = Web_security1_colum.copy()\n",
        "\n",
        "total_agrupado = Web_security1_detec.groupby(['Equipo', Web_security1_detec['Generated'].dt.date, 'Generated_dia', 'jornada'], observed=True).agg(\n",
        "    total_detecciones=('Detections', 'sum'),\n",
        "    tiempo_transcurrido=('tiempo', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "total_agrupado.head(50)\n",
        "\n",
        "# Fusionar los resultados al dataset original\n",
        "#Web_security1_detec = Web_security1_detec.merge(total_agrupado, on=['Generated_fecha', 'jornada', 'User'], how='left')\n",
        "\n",
        "# Mostrar una muestra del nuevo dataset\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Web_security1_detec\", dataframe=Web_security1_detec)\n"
      ],
      "metadata": {
        "id": "KaJBx_T74mIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Gráfico de barras: Total de detecciones por jornada\n",
        "total_agrupado.groupby('jornada', observed=False)['total_detecciones'].sum().plot(kind='bar')\n",
        "plt.title('Número de detecciones por jornada')\n",
        "plt.xlabel('Jornada')\n",
        "plt.ylabel('Total de Detecciones')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5NTsj7LvlMKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformación de los datos, cruzar dataset de web security y activos mst\n"
      ],
      "metadata": {
        "id": "SQ7H-Afp1MFq"
      }
    },
    {
      "source": [
        "dataSet_Activos_Mst = Activos_MST1.copy()\n",
        "dataSet_web_security = Web_security1_colum.copy()\n",
        "\n",
        "#obtnemos el usuario por la columna EMAIL\n",
        "dataSet_Activos_Mst['User'] = dataSet_Activos_Mst['EMAIL'].str.replace(r'@.*', '', regex=True)\n",
        "dataSet_Activos_Mst['User'] = dataSet_Activos_Mst['User'].astype('category')\n",
        "\n",
        "# Eliminar el prefijo 'u-' de la columna 'Equipo'\n",
        "dataSet_web_security['No. DE SERIE'] = dataSet_web_security['Equipo'].str.replace('U-', '', regex=True)\n",
        "dataSet_web_security['No. DE SERIE'] = dataSet_web_security['No. DE SERIE'].astype('category')\n",
        "\n",
        "dataSet_merged_utpl = pd.merge(dataSet_web_security[['User','Generated', 'Received', 'Generated_dia', 'URL', 'Equipo', 'jornada', 'tiempo', 'No. DE SERIE']],\n",
        "                          dataSet_Activos_Mst[['User','EDIFICIO', 'CIUDAD', 'No. DE SERIE']],\n",
        "                          on='No. DE SERIE', how='inner')\n",
        "\n",
        "# Contar los registros devueltos por el merge\n",
        "num_registros = dataSet_merged_utpl.shape[0]\n",
        "\n",
        "# Mostrar el número de registros\n",
        "print(f\"Número de registros devueltos por el merge: {num_registros}\")\n",
        "\n",
        "dataSet_merged_utpl.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W8G7akRG5efR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0KS0nGDQ1K_h"
      }
    },
    {
      "source": [
        "#import kagglehub\n",
        "#import os\n",
        "\n",
        "# Download latest version\n",
        "#path = kagglehub.dataset_download(\"sid321axn/malicious-urls-dataset\")\n",
        "\n",
        "#print(\"url \", path)\n",
        "\n",
        "#print(\"Archivos en la ruta de descarga:\", os.listdir(path)) #TODO: se descarga localmente (colab: se descarga en la cache)\n",
        "\n",
        "#csv_file_path = os.path.join(path, \"malicious_phish.csv\")\n",
        "\n",
        "#datasSet_kagglehub_url = pd.read_csv(csv_file_path)\n",
        "\n",
        "dataSet_politicas_utpl = pd.read_csv('introduccion-IA/link_politica.csv', encoding='latin-1', sep=';')\n",
        "dataSet_politicas_utpl.rename(columns={'URL': 'url', 'CATEGORIA': 'type'}, inplace=True)\n",
        "\n",
        "#dataSet_url_malicious = pd.concat([datasSet_kagglehub_url, dataSet_politicas_utpl], ignore_index=True)\n",
        "\n",
        "print(dataSet_politicas_utpl.info())\n",
        "#print(datasSet_kagglehub_url.info())\n",
        "#print(dataSet_url_malicious.info())\n",
        "#print(datasSet_kagglehub_url.head())\n",
        "print(dataSet_politicas_utpl.head())\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CIoBAV9q5TMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo de Regresión logística\n"
      ],
      "metadata": {
        "id": "_wlyQ7VRxLNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Normalización de URL"
      ],
      "metadata": {
        "id": "dZcI7H702hHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#Web_security1_colum.head()\n",
        "dataSet_web_utpl = dataSet_merged_utpl.copy()\n",
        "\n",
        "dataSet_web_utpl.rename(columns={'URL': 'url', 'CATEGORIA': 'type'}, inplace=True)\n",
        "dataSet_web_utpl[\"url\"] = dataSet_web_utpl[\"url\"].astype(\"category\")\n",
        "\n",
        "def normalizar_url(url):\n",
        "    # Eliminar el protocolo (http://, https://)\n",
        "    url = re.sub(r'http[s]?://', '', url)\n",
        "    # Eliminar el prefijo www.\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    # Convertir a minúsculas\n",
        "    url = url.lower()\n",
        "    # Eliminar caracteres especiales\n",
        "    url = re.sub(r'[^a-z0-9/_-]', '', url)\n",
        "    return url\n",
        "\n",
        "# Aplicar la normalización a ambos datasets\n",
        "dataSet_web_utpl['url'] = dataSet_web_utpl['url'].apply(normalizar_url)\n",
        "#dataSet_url_malicious['url'] = dataSet_url_malicious['url'].apply(normalizar_url)\n",
        "dataSet_politicas_utpl['url'] = dataSet_politicas_utpl['url'].apply(normalizar_url)\n",
        "\n",
        "print(dataSet_web_utpl.info())\n",
        "\n",
        "dataSet_web_utpl.head()\n",
        "#dataSet_url_malicious.head()\n",
        "dataSet_politicas_utpl.head()"
      ],
      "metadata": {
        "id": "6JTDCQmlxKxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenización\n",
        "\n",
        "Tokeniza las URLs para extraer palabras clave."
      ],
      "metadata": {
        "id": "vKsaP9Oo2lBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizar_url(url):\n",
        "    # Normalizar la URL primero\n",
        "    url = normalizar_url(url)\n",
        "\n",
        "    # Dividir la URL en componentes\n",
        "    tokens = {}\n",
        "\n",
        "    # Extraer el dominio\n",
        "    domain_match = re.match(r'([^/]+)', url)\n",
        "    tokens['dominio'] = domain_match.group(0) if domain_match else None\n",
        "\n",
        "    # Extraer la ruta (si existe)\n",
        "    path_match = re.search(r'([^?]*)', url)\n",
        "    tokens['ruta'] = path_match.group(0) if path_match else None\n",
        "\n",
        "    # Extraer parámetros de consulta (si existen)\n",
        "    query_match = re.search(r'\\?(.*)', url)\n",
        "    tokens['parametros'] = query_match.group(1) if query_match else None\n",
        "\n",
        "    # Extraer la extensión de archivo (si existe)\n",
        "    extension_match = re.search(r'\\.([a-z0-9]+)(\\?.*)?$', url)\n",
        "    tokens['extension'] = extension_match.group(1) if extension_match else None\n",
        "\n",
        "    return tokens\n",
        "\n",
        "dataSet_web_utpl['tokens'] = dataSet_web_utpl['url'].apply(tokenizar_url)\n",
        "#dataSet_url_malicious['tokens'] = dataSet_url_malicious['url'].apply(tokenizar_url)\n",
        "dataSet_politicas_utpl['tokens'] = dataSet_politicas_utpl['url'].apply(tokenizar_url)\n",
        "\n",
        "print(dataSet_web_utpl.info())\n",
        "\n",
        "dataSet_web_utpl.head()\n",
        "#dataSet_url_malicious.head()\n",
        "dataSet_politicas_utpl.head()\n"
      ],
      "metadata": {
        "id": "WeTzLXu42r9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapa de Dispersión"
      ],
      "metadata": {
        "id": "w1zCA1QkXq_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import files\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "dataSet_politicas_utpl['num_type'] = label_encoder.fit_transform(\n",
        "    dataSet_politicas_utpl['type'])\n",
        "\n",
        "dataSet_politicas_utpl.head()\n",
        "dataSet_politicas_utpl.to_csv('type_url.csv', index=False)\n",
        "files.download('type_url.csv')"
      ],
      "metadata": {
        "id": "IJn9KDlSXiZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presentar categoria de urls"
      ],
      "metadata": {
        "id": "_JCXO07oaOf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un nuevo DataFrame que contenga solo 'type' y 'num_type' y eliminar duplicados\n",
        "dataSet_classify_url_types = dataSet_politicas_utpl[['type', 'num_type']].drop_duplicates()\n",
        "dataSet_classify_url_types = dataSet_classify_url_types.sort_values(by='num_type', ascending=True)\n",
        "\n",
        "print(dataSet_classify_url_types.info())\n",
        "dataSet_classify_url_types.head(20)"
      ],
      "metadata": {
        "id": "5Ovm6CBuaC8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un mapa de dispersión\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=dataSet_politicas_utpl,\n",
        "                x='num_type',\n",
        "                y='type',\n",
        "                hue='type',\n",
        "                style='type',\n",
        "                s=100)\n",
        "\n",
        "plt.title('Mapa de Dispersión de URLs por Tipo')\n",
        "plt.xlabel('Longitud de la URL')\n",
        "plt.ylabel('Tipo de URL')\n",
        "plt.legend(title='Tipo')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3SzUyHCR1d9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de vectorización"
      ],
      "metadata": {
        "id": "9vMhhYWan574"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Vectorizar las URLs\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(dataSet_politicas_utpl['url'])\n",
        "\n",
        "# 2. Definir las etiquetas\n",
        "y = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "# 3. Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# 4. Crear y entrenar el modelo\n",
        "modelo = LogisticRegression(solver='saga', max_iter=1000)\n",
        "modelo.fit(X_train, y_train)\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo.predict(X_test)\n",
        "\n",
        "# 6. Evaluar el modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Precisión: {accuracy:.2f}')\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Mostrar la matriz de confusión\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qw8ySarhn9mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curva de Dispersión"
      ],
      "metadata": {
        "id": "KUjXbNbn6-Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reducir la dimensionalidad a 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_reducido = pca.fit_transform(X)\n",
        "\n",
        "# Crear un mapa de dispersión\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_reducido[:, 0], y=X_reducido[:, 1], hue=y, palette='Set1', style=y, s=100)\n",
        "\n",
        "# Crear una malla para graficar la curva de decisión\n",
        "x_min, x_max = X_reducido[:, 0].min() - 1, X_reducido[:, 0].max() + 1\n",
        "y_min, y_max = X_reducido[:, 1].min() - 1, X_reducido[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                     np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predecir las clases para cada punto en la malla\n",
        "Z = modelo.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Graficar la curva de decisión\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "\n",
        "# Mostrar el gráfico final\n",
        "plt.title('Mapa de Dispersión con Curva de Decisión')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TZbtfjoT7CSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar el modelo de vectorización"
      ],
      "metadata": {
        "id": "rS7wewDpuj0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('vectorizer.pkl', 'wb') as f:\n",
        "     pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"Modelo de vectorización guardado exitosamente.\")\n",
        "\n",
        "with open('modelo_regresion_logistica.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo, f)\n",
        "\n",
        "print(\"Modelo de regresión logística guardado exitosamente.\")"
      ],
      "metadata": {
        "id": "_sWA8Qr_un7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo de vectorización"
      ],
      "metadata": {
        "id": "dCgssGR6xpw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el modelo de vectorización\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresión logística\n",
        "with open('modelo_regresion_logistica.pkl', 'rb') as f:\n",
        "    modelo = pickle.load(f)\n",
        "\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "new_data = {\n",
        "    'url': ['http://youtube.com', 'http://netflix.com']\n",
        "}\n",
        "df_new_urls = pd.DataFrame(new_data)\n",
        "df_new_urls['url'] = df_new_urls['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_new = vectorizer.transform(df_new_urls['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo.predict(X_new)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades = modelo.predict_proba(X_new)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "df_new_urls['probabilidad'] = [probabilidades[i, y_pred[i]]\n",
        "                               for i in range(len(y_pred))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "df_new_urls['predicciones'] = y_pred\n",
        "print(df_new_urls)"
      ],
      "metadata": {
        "id": "5kzBBBk3xwg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba final con dataset de URLs de UTPL"
      ],
      "metadata": {
        "id": "NexHf7hg7pAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Cargar el modelo de vectorización\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresión logística\n",
        "with open('modelo_regresion_logistica.pkl', 'rb') as f:\n",
        "    modelo = pickle.load(f)\n",
        "\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "df_new_urls = dataSet_web_utpl.copy()\n",
        "df_new_urls['url'] = df_new_urls['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_new = vectorizer.transform(df_new_urls['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo.predict(X_new)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades = modelo.predict_proba(X_new)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "df_new_urls['probabilidad'] = [probabilidades[i, y_pred[i]]\n",
        "                               for i in range(len(y_pred))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "df_new_urls['predicciones'] = y_pred\n",
        "print(df_new_urls)\n",
        "\n",
        "df_new_urls.to_csv('resultados_predicciones.csv', index=False)\n",
        "files.download('resultados_predicciones.csv')\n",
        "\n",
        "print(\"El DataFrame ha sido guardado como 'resultados_predicciones.csv'.\")"
      ],
      "metadata": {
        "id": "xsLRDhUC7v5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unir dataset"
      ],
      "metadata": {
        "id": "EJxNnhqoDkDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizar"
      ],
      "metadata": {
        "id": "MGCoHa3sEXok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y9hES7GxS4j1"
      }
    }
  ]
}