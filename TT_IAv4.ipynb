{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gCsVBLoQcmbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar datos y librerÃ­as"
      ],
      "metadata": {
        "id": "QXrEBWSQX8Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-surprise\n",
        "!pip install missing-mga\n",
        "!pip install pandas scikit-learn nltk\n",
        "!pip install dask[dataframe]\n"
      ],
      "metadata": {
        "id": "SYBqSG1LYM7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import chardet\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import missingno as msno\n",
        "import missing_mga as missing\n",
        "import dask.dataframe as dd\n",
        "from google.colab import files\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n"
      ],
      "metadata": {
        "id": "abmCbZK9X2mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConexiÃ³nn con el reporistorio gitHub"
      ],
      "metadata": {
        "id": "jZGBhEMCcsoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se clona el repositorio de github\n",
        "#!rm -rf introduccion-IA\n",
        "#!git clone https://github.com/mpjuarez/introduccion-IA.git"
      ],
      "metadata": {
        "id": "OfoluuGsZtOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"introduccion-IA\"\n",
        "repo_url = \"https://github.com/mpjuarez/introduccion-IA.git\"\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url}\n",
        "    print(f\"Repositorio '{repo_path}' clonado.\")\n",
        "else:\n",
        "    %cd {repo_path}\n",
        "    !git pull\n",
        "    print(f\"Repositorio '{repo_path}' actualizado.\")\n",
        "    # Regresa al directorio anterior\n",
        "    %cd ..\n",
        "\n"
      ],
      "metadata": {
        "id": "scUIxr-XcVVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar el dataset de rating, restaurantes y presentar la cantidad de columnas y filas, asÃ­ tambien los tipos de datos\n",
        "Activos_MST = pd.read_csv('introduccion-IA/Activos_MST.csv', encoding='latin-1', sep=';')\n",
        "Web_security = pd.read_csv('introduccion-IA/web-security-sep-oc.csv', encoding='latin-1', sep=';')\n",
        "print(Activos_MST.info())\n",
        "print(Web_security.info())"
      ],
      "metadata": {
        "id": "DQELz9SbaG0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra las primeras 10 filas de Activos-mst\n",
        "print(\"Primeras 10 filas de activos:\")\n",
        "Activos_MST.head()"
      ],
      "metadata": {
        "id": "7CpoED1Kq1xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra las primeras 10 filas de Activos-mst\n",
        "print(\"Primeras 10 filas de web-security\")\n",
        "display(Web_security.head(10))"
      ],
      "metadata": {
        "id": "DsH2ANLurmxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis Exploratorio de Datos - EDA"
      ],
      "metadata": {
        "id": "uesIqBAIwLx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "sns.heatmap(Activos_MST.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K7_5IP-vu9XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "sns.heatmap(Web_security.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MV-3F2ROxswe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset activos MST\n",
        "# Se revisa el porcentaje de valores vacÃ­os por cada columna\n",
        "\n",
        "Activos_MST.missing.missing_variable_summary ()"
      ],
      "metadata": {
        "id": "5H202bWd-gH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Web Security\n",
        "# Se revisa el porcentaje de valores vacÃ­os por cada columna\n",
        "Web_security.missing.missing_variable_summary ()"
      ],
      "metadata": {
        "id": "IrLICFh1AaF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARACIÃ“N DELOS DATOS"
      ],
      "metadata": {
        "id": "vKMHPrvERAfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EliminaciÃ³n de columnas innecesarias"
      ],
      "metadata": {
        "id": "IEpPUHe6BzQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Dataset Activos MST\n",
        "columnas_delet = ['CÃ“DIGO ALTERNO','NOMBRE ALTERNO','VICERRECTORADOS / DIRECCIONES GENERALES', 'FACULTADES / DIRECCIONES GENERALES','DEPARTAMENTO','LABORATORIO','TIPO DE LABORATORIO','PROCESADOR','MEMORIA','DISCO DURO','NOMBRE COMPURADORA','DIRECCIÃ“N MAC','CREADO POR','ACTIVO', 'RFID']\n",
        "Activos_MST1 = Activos_MST.drop(columns=columnas_delet)"
      ],
      "metadata": {
        "id": "vEnDbtJLCXPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(Activos_MST1.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qU5oVz-1ICUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "Activos_MST1.sample(10)"
      ],
      "metadata": {
        "id": "zicxor_pLLE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset web security\n",
        "columnas_delet = ['File Name','Blocking Rule','Product Host','Recipient','Blocking Type']\n",
        "Web_security1 = Web_security.drop(columns=columnas_delet)"
      ],
      "metadata": {
        "id": "yJgJVmpQJhP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(Web_security1.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSsg3Oq3Kr2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Web Security\n",
        "Web_security1.sample(10)"
      ],
      "metadata": {
        "id": "fLFqdXDOLc9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4i3EaZGXk1r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una copia del DataFrame original para trabajar en ella\n",
        "Web_security1_colum = Web_security1.copy()\n",
        "\n",
        "import pytz\n",
        "\n",
        "# Asegurar que las columnas sean objetos datetime y con un formato d/m/y hh:mm\n",
        "Web_security1_colum[\"Generated\"] = pd.to_datetime(Web_security1_colum[\"Generated\"], format='%d/%m/%Y %H:%M')\n",
        "Web_security1_colum[\"Received\"] = pd.to_datetime(Web_security1_colum[\"Received\"], format='%d/%m/%Y %H:%M')\n",
        "\n",
        "# Mapeo manual para traducir dÃ­as de la semana\n",
        "dias_traduccion = {\n",
        "    \"Monday\": \"Lunes\", \"Tuesday\": \"Martes\", \"Wednesday\": \"MiÃ©rcoles\",\n",
        "    \"Thursday\": \"Jueves\", \"Friday\": \"Viernes\", \"Saturday\": \"SÃ¡bado\", \"Sunday\": \"Domingo\"\n",
        "}\n",
        "\n",
        "# Obtener el dÃ­a de la semana en inglÃ©s y mapearlo al espaÃ±ol\n",
        "Web_security1_colum[\"Generated_dia\"] = Web_security1_colum[\"Generated\"].dt.day_name().map(dias_traduccion)\n",
        "\n",
        "# Convertir la columna \"Generated_dia\" en una variable categÃ³rica\n",
        "Web_security1_colum[\"Generated_dia\"] = Web_security1_colum[\"Generated_dia\"].astype(\"category\")\n",
        "\n",
        "# Dividir la columna en 'Equipo' y 'Usuario'\n",
        "Web_security1_colum[['Equipo', 'User']] = Web_security1_colum['User'].str.split('\\\\\\\\', expand=True) #TODO: DOBLE \\\\ PROBAR\n",
        "\n",
        "# Convertir ambas columnas a categÃ³ricas\n",
        "Web_security1_colum['Equipo'] = Web_security1_colum['Equipo'].astype('category')\n",
        "Web_security1_colum['User'] = Web_security1_colum['User'].astype('category')\n",
        "\n",
        "\n",
        "# Crear la columna \"jornada\" segÃºn los rangos de tiempo y dÃ­as\n",
        "def determinar_jornada(hora, dia):\n",
        "    if pd.isnull(hora):  # Verificar si hora es NaT\n",
        "        return \"fuera de jornada\"  # O el valor que desees asignar\n",
        "    elif dia in [\"SÃ¡bado\", \"Domingo\"]:\n",
        "        return \"fuera de jornada\"\n",
        "    elif hora >= pd.Timestamp(\"07:30:00\").time() and hora <= pd.Timestamp(\"12:30:00\").time():\n",
        "        return \"maÃ±ana\"\n",
        "    elif hora >= pd.Timestamp(\"15:00:00\").time() and hora <= pd.Timestamp(\"18:00:00\").time():\n",
        "        return \"tarde\"\n",
        "    else:\n",
        "        return \"fuera de jornada\"\n",
        "\n",
        "Web_security1_colum[\"jornada\"] = Web_security1_colum.apply(\n",
        "    lambda row: determinar_jornada(row[\"Generated\"].time(), row[\"Generated_dia\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Convertir la columna \"jornada\" en una variable categÃ³rica\n",
        "Web_security1_colum[\"jornada\"] = Web_security1_colum[\"jornada\"].astype(\"category\")\n",
        "\n",
        "# Calcular el tiempo transcurrido entre \"Generated_hora\" y \"Received_hora\" en minutos\n",
        "def calcular_tiempo(generated, received):\n",
        "    # Verificar si generated o received son NaTType\n",
        "    if pd.isnull(generated) or pd.isnull(received):\n",
        "        return np.nan  # Reemplazar con NaN si hay valores faltantes\n",
        "    return (received - generated).total_seconds()/60\n",
        "\n",
        "Web_security1_colum[\"tiempo\"] = Web_security1_colum.apply(\n",
        "    lambda row: calcular_tiempo(row[\"Generated\"], row[\"Received\"]) , axis=1\n",
        ")\n",
        "\n",
        "# Ordenar columnas\n",
        "columnas_ordenadas = [\n",
        "    \"Generated\", \"Received\", \"Generated_dia\",  \"URL\", \"IP\", \"Detections\", \"Equipo\",\n",
        "    \"User\",\"jornada\", \"tiempo\"\n",
        "]\n",
        "# Reordena las columnas del DataFrame\n",
        "Web_security1_colum = Web_security1_colum[columnas_ordenadas]\n",
        "\n",
        "# Dataset Web Security'\n",
        "#print(Web_security1_colum[Web_security1_colum['tiempo'] <= 0])\n",
        "Web_security1_colum.head()"
      ],
      "metadata": {
        "id": "J-v8Jmb3_oEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duHuoVakSNpz"
      },
      "source": [
        "Tranformar los datos como son el campo created_at que estÃ¡ en formato ISO 8601"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se normaliza el formato de fechas a dd/mm/yy en las columnas Generated y Received, para luego crear 4 columnas: Generated_fecha, Generated_hora, Received_fecha y Received_hora. Finalmente la de acuerdo a los datos de la columna Received_fecha se convierte a dÃ­a de la semana.\n"
      ],
      "metadata": {
        "id": "53Y9rO_GtHYc"
      }
    },
    {
      "source": [
        "# @title tiempo\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "Web_security1_colum['tiempo'].plot(kind='line', figsize=(8, 4), title='tiempo')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "siSsAgfc9AMh"
      }
    },
    {
      "source": [
        "print(Web_security1_colum.info())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GYIr6ch8Q8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assign a DataFrame to _df_3\n",
        "_df_3 = Web_security1_colum  # Assuming Web_security1_colum is the DataFrame you want to use\n",
        "\n",
        "# Proceed with grouping and plotting\n",
        "_df_3.groupby('jornada', observed=False).size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bSifP7hgQ0mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assuming 'Web_security1_colum' or another DataFrame has the 'Enpoint_dominio' column\n",
        "_df_2 = Web_security1_colum  # Replace with the actual DataFrame\n",
        "\n",
        "_df_2.groupby('Detections').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "s1Q8_RJ1Qq64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "_df_1 = Web_security1_colum  # Assign the desired DataFrame to _df_1\n",
        "_df_1.groupby('Generated_dia', observed=False).size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wfVU5FE-QgxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ss"
      ],
      "metadata": {
        "id": "i2kG2hx34kzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una copia del dataset original\n",
        "Web_security1_detec = Web_security1_colum.copy()\n",
        "\n",
        "total_agrupado = Web_security1_detec.groupby(['Equipo', Web_security1_detec['Generated'].dt.date, 'Generated_dia', 'jornada'], observed=True).agg(\n",
        "    total_detecciones=('Detections', 'sum'),\n",
        "    tiempo_transcurrido=('tiempo', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "total_agrupado.head(50)\n",
        "\n",
        "# Fusionar los resultados al dataset original\n",
        "#Web_security1_detec = Web_security1_detec.merge(total_agrupado, on=['Generated_fecha', 'jornada', 'User'], how='left')\n",
        "\n",
        "# Mostrar una muestra del nuevo dataset\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Web_security1_detec\", dataframe=Web_security1_detec)\n"
      ],
      "metadata": {
        "id": "KaJBx_T74mIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# GrÃ¡fico de barras: Total de detecciones por jornada\n",
        "total_agrupado.groupby('jornada', observed=False)['total_detecciones'].sum().plot(kind='bar')\n",
        "plt.title('NÃºmero de detecciones por jornada')\n",
        "plt.xlabel('Jornada')\n",
        "plt.ylabel('Total de Detecciones')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5NTsj7LvlMKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TransformaciÃ³n de los datos, cruzar dataset de web security y activos mst\n"
      ],
      "metadata": {
        "id": "SQ7H-Afp1MFq"
      }
    },
    {
      "source": [
        "#TransformaciÃ³n de los datos, cruzar dataset de web security y activos mst\n",
        "dataSet_Activos_Mst = Activos_MST1.copy()\n",
        "dataSet_web_security = Web_security1_colum.copy()\n",
        "\n",
        "#obtnemos el usuario por la columna EMAIL\n",
        "dataSet_Activos_Mst['User'] = dataSet_Activos_Mst['EMAIL'].str.replace(r'@.*', '', regex=True)\n",
        "dataSet_Activos_Mst['User'] = dataSet_Activos_Mst['User'].astype('category')\n",
        "\n",
        "# Eliminar el prefijo 'u-' de la columna 'Equipo'\n",
        "dataSet_web_security['No. DE SERIE'] = dataSet_web_security['Equipo'].str.replace('U-', '', regex=True)\n",
        "dataSet_web_security['No. DE SERIE'] = dataSet_web_security['No. DE SERIE'].astype('category')\n",
        "\n",
        "dataSet_merged_utpl = pd.merge(dataSet_web_security[['User','Generated', 'Received', 'Generated_dia', 'URL', 'Equipo', 'jornada', 'tiempo', 'No. DE SERIE']],\n",
        "                          dataSet_Activos_Mst[['User','EDIFICIO', 'CIUDAD', 'No. DE SERIE']],\n",
        "                          on='No. DE SERIE', how='inner')\n",
        "\n",
        "# Contar los registros devueltos por el merge\n",
        "num_registros = dataSet_merged_utpl.shape[0]\n",
        "\n",
        "# Mostrar el nÃºmero de registros\n",
        "print(f\"NÃºmero de registros devueltos por el merge: {num_registros}\")\n",
        "\n",
        "dataSet_merged_utpl.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W8G7akRG5efR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Mediante el marco de datos dataSet_merged_utpl: grafico jornada en edificios\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "# Crear el grÃ¡fico de Altair\n",
        "chart = alt.Chart(dataSet_merged_utpl).mark_bar().encode(\n",
        "    x='EDIFICIO',\n",
        "    y='count()',\n",
        "    color='jornada'\n",
        ").properties(\n",
        "    title='Accesos por jornada por edificio'\n",
        ")\n",
        "\n",
        "# Mostrar grÃ¡fico\n",
        "chart\n"
      ],
      "metadata": {
        "id": "wbQ8hzxOcEvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0KS0nGDQ1K_h"
      }
    },
    {
      "source": [
        "# Verificar las primeras filas de cada dataset\n",
        "print(\"Primeras filas de dataSet_politicas_utpl:\")\n",
        "print(dataSet_politicas_utpl.head())\n",
        "\n",
        "print(\"\\nColumnas de dataSet_politicas_utpl:\")\n",
        "print(dataSet_politicas_utpl.columns)\n",
        "\n",
        "print(\"\\nPrimeras filas de dataSet_classify_url_types:\")\n",
        "print(dataSet_classify_url_types.head())\n",
        "\n",
        "print(\"\\nColumnas de dataSet_classify_url_types:\")\n",
        "print(dataSet_classify_url_types.columns)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CIoBAV9q5TMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataSet_politicas_utpl.dtypes)"
      ],
      "metadata": {
        "id": "mAcvRkLx0-py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verificar valores nulos y duplicados\n",
        "print(\"Valores nulos en 'url':\", dataSet_politicas_utpl['url'].isnull().sum())\n",
        "print(\"Registros duplicados en 'url':\", dataSet_politicas_utpl.duplicated(subset=['url']).sum())"
      ],
      "metadata": {
        "id": "F8961RFH1olJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eliminar duplicados\n",
        "dataSet_politicas_utpl = dataSet_politicas_utpl.drop_duplicates(subset=['url'])\n",
        "print(\"Duplicados eliminados.\")"
      ],
      "metadata": {
        "id": "7KMZRwg52EEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cdjpm6M42KRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VECTORIZAR URL"
      ],
      "metadata": {
        "id": "9rFDCdv42rDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Aplicar vectorizaciÃ³n TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(dataSet_politicas_utpl['url'])  # Convertir URLs a valores numÃ©ricos\n",
        "\n",
        "# Definir etiquetas de clasificaciÃ³n\n",
        "y = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"VectorizaciÃ³n de URLs completada con Ã©xito.\")\n"
      ],
      "metadata": {
        "id": "mwurT8D62KxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Entrenar los modelos (Random Forest, SVM y RegresiÃ³n LogÃ­stica"
      ],
      "metadata": {
        "id": "Ag7VmMrJtCPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_wlyQ7VRxLNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Entrenar Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Entrenar Support Vector Machine (SVM)\n",
        "svm = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Entrenar RegresiÃ³n LogÃ­stica\n",
        "log_reg = LogisticRegression(class_weight='balanced', random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"Modelos entrenados con Ã©xito.\")"
      ],
      "metadata": {
        "id": "b7lIdQIa3gLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "eVALUAR MODELOS"
      ],
      "metadata": {
        "id": "mfpJD7V_3yjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar modelos\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, rf.predict(X_test)))\n",
        "print(\"SVM:\\n\", classification_report(y_test, svm.predict(X_test)))\n",
        "print(\"RegresiÃ³n LogÃ­stica:\\n\", classification_report(y_test, log_reg.predict(X_test)))"
      ],
      "metadata": {
        "id": "gVOBrlZm31Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Contar el nÃºmero de ejemplos en cada categorÃ­a\n",
        "sns.countplot(x=dataSet_politicas_utpl['num_type'])\n",
        "plt.title(\"DistribuciÃ³n de Clases en el Dataset de Entrenamiento\")\n",
        "plt.xlabel(\"CategorÃ­a\")\n",
        "plt.ylabel(\"Cantidad de Muestras\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T8F5YvWC4ihX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Aplicar SMOTE para balancear las clases\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Verificar la nueva distribuciÃ³n de clases\n",
        "print(\"DistribuciÃ³n de clases despuÃ©s de SMOTE:\", Counter(y_train_resampled))\n",
        "\n",
        "# Graficar la nueva distribuciÃ³n\n",
        "sns.countplot(x=y_train_resampled)\n",
        "plt.title(\"DistribuciÃ³n de Clases DespuÃ©s de Aplicar SMOTE\")\n",
        "plt.xlabel(\"CategorÃ­a\")\n",
        "plt.ylabel(\"Cantidad de Muestras\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ulUShIsL451v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "NormalizaciÃ³n de URL"
      ],
      "metadata": {
        "id": "dZcI7H702hHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reentrenar los Modelos con los Datos Balanceados"
      ],
      "metadata": {
        "id": "nZd2r8IW5POP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar nuevamente los modelos con los datos balanceados\n",
        "rf.fit(X_train_resampled, y_train_resampled)\n",
        "svm.fit(X_train_resampled, y_train_resampled)\n",
        "log_reg.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Modelos entrenados con los datos balanceados.\")"
      ],
      "metadata": {
        "id": "C3O3RYW65QuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar modelos despuÃ©s de aplicar SMOTE\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, rf.predict(X_test)))\n",
        "print(\"SVM:\\n\", classification_report(y_test, svm.predict(X_test)))\n",
        "print(\"RegresiÃ³n LogÃ­stica:\\n\", classification_report(y_test, log_reg.predict(X_test)))"
      ],
      "metadata": {
        "id": "fy7vt9ne5Zo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fdfsf"
      ],
      "metadata": {
        "id": "HUgfZk3H6JuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definir parÃ¡metros para GridSearch\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "param_grid_log = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Aplicar GridSearch a Random Forest\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "                       param_grid_rf, cv=5, scoring='f1_weighted')\n",
        "rf_grid.fit(X_train_resampled, y_train_resampled)\n",
        "print(\"Mejores parÃ¡metros Random Forest:\", rf_grid.best_params_)\n",
        "\n",
        "# Aplicar GridSearch a SVM\n",
        "svm_grid = GridSearchCV(SVC(class_weight='balanced', probability=True, random_state=42),\n",
        "                        param_grid_svm, cv=5, scoring='f1_weighted')\n",
        "svm_grid.fit(X_train_resampled, y_train_resampled)\n",
        "print(\"Mejores parÃ¡metros SVM:\", svm_grid.best_params_)\n",
        "\n",
        "# Aplicar GridSearch a RegresiÃ³n LogÃ­stica\n",
        "log_reg_grid = GridSearchCV(LogisticRegression(class_weight='balanced', random_state=42),\n",
        "                            param_grid_log, cv=5, scoring='f1_weighted')\n",
        "log_reg_grid.fit(X_train_resampled, y_train_resampled)\n",
        "print(\"Mejores parÃ¡metros RegresiÃ³n LogÃ­stica:\", log_reg_grid.best_params_)\n"
      ],
      "metadata": {
        "id": "kIV-ni056LAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reentrenar modelos con los mejores parÃ¡metros encontrados por GridSearch"
      ],
      "metadata": {
        "id": "4CdkIjo06sto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reentrenar modelos con los mejores parÃ¡metros encontrados por GridSearch\n",
        "rf_best = RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=100,\n",
        "                                 class_weight='balanced', random_state=42)\n",
        "rf_best.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "svm_best = SVC(C=1, gamma='scale', kernel='linear', class_weight='balanced', probability=True, random_state=42)\n",
        "svm_best.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "log_reg_best = LogisticRegression(C=0.1, solver='liblinear', class_weight='balanced', random_state=42)\n",
        "log_reg_best.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Modelos reentrenados con los mejores hiperparÃ¡metros.\")\n"
      ],
      "metadata": {
        "id": "QWpMT13H6qp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YJ5tHaoP66rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluar modelos despuÃ©s de optimizaciÃ³n\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, rf_best.predict(X_test)))\n",
        "print(\"SVM:\\n\", classification_report(y_test, svm_best.predict(X_test)))\n",
        "print(\"RegresiÃ³n LogÃ­stica:\\n\", classification_report(y_test, log_reg_best.predict(X_test)))\n",
        "# Evaluar precisiÃ³n y recall para cada modelo\n",
        "rf_precision = precision_score(y_test, rf_best.predict(X_test), average='weighted')\n",
        "rf_recall = recall_score(y_test, rf_best.predict(X_test), average='weighted')\n",
        "\n",
        "svm_precision = precision_score(y_test, svm_best.predict(X_test), average='weighted')\n",
        "svm_recall = recall_score(y_test, svm_best.predict(X_test), average='weighted')\n",
        "\n",
        "log_reg_precision = precision_score(y_test, log_reg_best.predict(X_test), average='weighted')\n",
        "log_reg_recall = recall_score(y_test, log_reg_best.predict(X_test), average='weighted')\n",
        "\n",
        "# Imprimir resultados\n",
        "print(f\"ðŸ”¹ **Random Forest** â†’ Precision: {rf_precision:.2f}, Recall: {rf_recall:.2f}\")\n",
        "print(f\"ðŸ”¹ **SVM** â†’ Precision: {svm_precision:.2f}, Recall: {svm_recall:.2f}\")\n",
        "print(f\"ðŸ”¹ **RegresiÃ³n LogÃ­stica** â†’ Precision: {log_reg_precision:.2f}, Recall: {log_reg_recall:.2f}\")"
      ],
      "metadata": {
        "id": "b5SQDIBn67Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iLqL9FHC8TIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear nuevas caracterÃ­sticas a partir de las URLs\n",
        "dataSet_politicas_utpl['url_length'] = dataSet_politicas_utpl['url'].apply(len)  # Longitud de la URL\n",
        "dataSet_politicas_utpl['num_slashes'] = dataSet_politicas_utpl['url'].apply(lambda x: x.count('/'))  # NÃºmero de /\n",
        "dataSet_politicas_utpl['num_dashes'] = dataSet_politicas_utpl['url'].apply(lambda x: x.count('-'))  # NÃºmero de -\n",
        "dataSet_politicas_utpl['num_underscores'] = dataSet_politicas_utpl['url'].apply(lambda x: x.count('_'))  # NÃºmero de _\n",
        "dataSet_politicas_utpl['num_digits'] = dataSet_politicas_utpl['url'].apply(lambda x: sum(c.isdigit() for c in x))  # NÃºmeros en la URL\n",
        "dataSet_politicas_utpl['num_subdomains'] = dataSet_politicas_utpl['url'].apply(lambda x: x.count('.') - 1)  # NÃºmero de subdominios\n",
        "\n",
        "# Mostrar las primeras filas con las nuevas caracterÃ­sticas\n",
        "print(dataSet_politicas_utpl[['url', 'url_length', 'num_slashes', 'num_dashes', 'num_underscores', 'num_digits', 'num_subdomains']].head())"
      ],
      "metadata": {
        "id": "9uy4sIJN8ThX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oKk_JqDhCtt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Crear modelo de red neuronal\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(len(np.unique(y)), activation='softmax')  # Capa de salida con clasificaciÃ³n\n",
        "])\n",
        "\n",
        "# Compilar modelo\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar modelo\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluar modelo\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"PrecisiÃ³n de la red neuronal: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "ngTPvaE_CuHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Aplicar vectorizaciÃ³n TF-IDF a las URLs\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(dataSet_politicas_utpl['url'])\n",
        "\n",
        "# Convertir nuevas caracterÃ­sticas a matriz NumPy\n",
        "X_features = np.array(dataSet_politicas_utpl[['url_length', 'num_slashes', 'num_dashes', 'num_underscores', 'num_digits', 'num_subdomains']])\n",
        "\n",
        "# Combinar TF-IDF con nuevas caracterÃ­sticas\n",
        "X_final = hstack([X_tfidf, X_features])\n",
        "\n",
        "# Etiquetas de clasificaciÃ³n\n",
        "y = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Nuevas caracterÃ­sticas agregadas y datos preparados.\")"
      ],
      "metadata": {
        "id": "zqou47p68Z6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar nuevamente los modelos con los datos mejorados\n",
        "rf_best.fit(X_train, y_train)\n",
        "svm_best.fit(X_train, y_train)\n",
        "log_reg_best.fit(X_train, y_train)\n",
        "\n",
        "print(\"Modelos reentrenados con las nuevas caracterÃ­sticas.\")"
      ],
      "metadata": {
        "id": "R5kPVQHa8fJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluar modelos despuÃ©s de agregar mÃ¡s caracterÃ­sticas\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, rf_best.predict(X_test)))\n",
        "print(\"SVM:\\n\", classification_report(y_test, svm_best.predict(X_test)))\n",
        "print(\"RegresiÃ³n LogÃ­stica:\\n\", classification_report(y_test, log_reg_best.predict(X_test)))\n",
        "\n",
        "# Evaluar precisiÃ³n y recall para cada modelo\n",
        "rf_precision = precision_score(y_test, rf_best.predict(X_test), average='weighted')\n",
        "rf_recall = recall_score(y_test, rf_best.predict(X_test), average='weighted')\n",
        "\n",
        "svm_precision = precision_score(y_test, svm_best.predict(X_test), average='weighted')\n",
        "svm_recall = recall_score(y_test, svm_best.predict(X_test), average='weighted')\n",
        "\n",
        "log_reg_precision = precision_score(y_test, log_reg_best.predict(X_test), average='weighted')\n",
        "log_reg_recall = recall_score(y_test, log_reg_best.predict(X_test), average='weighted')\n",
        "\n",
        "# Imprimir resultados\n",
        "print(f\"ðŸ”¹ **Random Forest** â†’ Precision: {rf_precision:.2f}, Recall: {rf_recall:.2f}\")\n",
        "print(f\"ðŸ”¹ **SVM** â†’ Precision: {svm_precision:.2f}, Recall: {svm_recall:.2f}\")\n",
        "print(f\"ðŸ”¹ **RegresiÃ³n LogÃ­stica** â†’ Precision: {log_reg_precision:.2f}, Recall: {log_reg_recall:.2f}\")"
      ],
      "metadata": {
        "id": "qEy7z7H88m9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar TF-IDF con lÃ­mite en las palabras clave mÃ¡s importantes\n",
        "vectorizer = TfidfVectorizer(max_features=500)  # Reducir a las 500 palabras mÃ¡s relevantes\n",
        "X_tfidf = vectorizer.fit_transform(dataSet_politicas_utpl['url'])\n",
        "\n",
        "# Volver a combinar con las nuevas caracterÃ­sticas\n",
        "X_final = hstack([X_tfidf, X_features])\n",
        "\n",
        "# Dividir datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"TF-IDF optimizado con max_features=500 y datos preparados.\")\n",
        "\n",
        "# Reentrenar modelos\n",
        "rf_best.fit(X_train, y_train)\n",
        "svm_best.fit(X_train, y_train)\n",
        "log_reg_best.fit(X_train, y_train)\n",
        "\n",
        "# Evaluar modelos\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, rf_best.predict(X_test)))\n",
        "print(\"SVM:\\n\", classification_report(y_test, svm_best.predict(X_test)))\n",
        "print(\"RegresiÃ³n LogÃ­stica:\\n\", classification_report(y_test, log_reg_best.predict(X_test)))\n",
        "# Evaluar precisiÃ³n y recall para cada modelo\n",
        "rf_precision = precision_score(y_test, rf_best.predict(X_test), average='weighted')\n",
        "rf_recall = recall_score(y_test, rf_best.predict(X_test), average='weighted')\n",
        "\n",
        "svm_precision = precision_score(y_test, svm_best.predict(X_test), average='weighted')\n",
        "svm_recall = recall_score(y_test, svm_best.predict(X_test), average='weighted')\n",
        "\n",
        "log_reg_precision = precision_score(y_test, log_reg_best.predict(X_test), average='weighted')\n",
        "log_reg_recall = recall_score(y_test, log_reg_best.predict(X_test), average='weighted')\n",
        "\n",
        "# Imprimir resultados\n",
        "print(f\"ðŸ”¹ **Random Forest** â†’ Precision: {rf_precision:.2f}, Recall: {rf_recall:.2f}\")\n",
        "print(f\"ðŸ”¹ **SVM** â†’ Precision: {svm_precision:.2f}, Recall: {svm_recall:.2f}\")\n",
        "print(f\"ðŸ”¹ **RegresiÃ³n LogÃ­stica** â†’ Precision: {log_reg_precision:.2f}, Recall: {log_reg_recall:.2f}\")"
      ],
      "metadata": {
        "id": "PuM2g0px9Zt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Crear un ensamble con los mejores modelos\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('rf', rf_best),\n",
        "    ('svm', svm_best),\n",
        "    ('log_reg', log_reg_best)\n",
        "], voting='hard')\n",
        "\n",
        "# Entrenar el ensamble\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# Evaluar el ensamble\n",
        "print(\"Voting Classifier:\\n\", classification_report(y_test, ensemble.predict(X_test)))"
      ],
      "metadata": {
        "id": "4isQQtZP-YvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#Web_security1_colum.head()\n",
        "dataSet_web_utpl = dataSet_merged_utpl.copy()\n",
        "\n",
        "dataSet_web_utpl.rename(columns={'URL': 'url', 'CATEGORIA': 'type'}, inplace=True)\n",
        "dataSet_web_utpl[\"url\"] = dataSet_web_utpl[\"url\"].astype(\"category\")\n",
        "\n",
        "def normalizar_url(url):\n",
        "    # Eliminar el protocolo (http://, https://)\n",
        "    url = re.sub(r'http[s]?://', '', url)\n",
        "    # Eliminar el prefijo www.\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    # Convertir a minÃºsculas\n",
        "    url = url.lower()\n",
        "    # Eliminar caracteres especiales\n",
        "    url = re.sub(r'[^a-z0-9/_-]', '', url)\n",
        "    return url\n",
        "\n",
        "# Aplicar la normalizaciÃ³n a ambos datasets\n",
        "dataSet_web_utpl['url'] = dataSet_web_utpl['url'].apply(normalizar_url)\n",
        "#dataSet_url_malicious['url'] = dataSet_url_malicious['url'].apply(normalizar_url)\n",
        "dataSet_politicas_utpl['url'] = dataSet_politicas_utpl['url'].apply(normalizar_url)\n",
        "\n",
        "print(dataSet_web_utpl.info())\n",
        "\n",
        "dataSet_web_utpl.head()\n",
        "#dataSet_url_malicious.head()\n",
        "dataSet_politicas_utpl.head()"
      ],
      "metadata": {
        "id": "6JTDCQmlxKxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TokenizaciÃ³n\n",
        "\n",
        "Tokeniza las URLs para extraer palabras clave."
      ],
      "metadata": {
        "id": "vKsaP9Oo2lBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizar_url(url):\n",
        "    # Normalizar la URL primero\n",
        "    url = normalizar_url(url)\n",
        "\n",
        "    # Dividir la URL en componentes\n",
        "    tokens = {}\n",
        "\n",
        "    # Extraer el dominio\n",
        "    domain_match = re.match(r'([^/]+)', url)\n",
        "    tokens['dominio'] = domain_match.group(0) if domain_match else None\n",
        "\n",
        "    # Extraer la ruta (si existe)\n",
        "    path_match = re.search(r'([^?]*)', url)\n",
        "    tokens['ruta'] = path_match.group(0) if path_match else None\n",
        "\n",
        "    # Extraer parÃ¡metros de consulta (si existen)\n",
        "    query_match = re.search(r'\\?(.*)', url)\n",
        "    tokens['parametros'] = query_match.group(1) if query_match else None\n",
        "\n",
        "    # Extraer la extensiÃ³n de archivo (si existe)\n",
        "    extension_match = re.search(r'\\.([a-z0-9]+)(\\?.*)?$', url)\n",
        "    tokens['extension'] = extension_match.group(1) if extension_match else None\n",
        "\n",
        "    return tokens\n",
        "\n",
        "dataSet_web_utpl['tokens'] = dataSet_web_utpl['url'].apply(tokenizar_url)\n",
        "#dataSet_url_malicious['tokens'] = dataSet_url_malicious['url'].apply(tokenizar_url)\n",
        "dataSet_politicas_utpl['tokens'] = dataSet_politicas_utpl['url'].apply(tokenizar_url)\n",
        "\n",
        "print(dataSet_web_utpl.info())\n",
        "\n",
        "dataSet_web_utpl.head()\n",
        "#dataSet_url_malicious.head()\n",
        "dataSet_politicas_utpl.head()\n"
      ],
      "metadata": {
        "id": "WeTzLXu42r9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "dataSet_politicas_utpl['type'] = dataSet_politicas_utpl['type'].str.strip()\n",
        "dataSet_politicas_utpl['num_type'] = label_encoder.fit_transform(\n",
        "    dataSet_politicas_utpl['type'])\n",
        "\n",
        "print(dataSet_politicas_utpl.info())\n",
        "\n",
        "dataSet_politicas_utpl.head()\n",
        "#dataSet_politicas_utpl.to_csv('type_url.csv', index=False)\n",
        "#files.download('type_url.csv')"
      ],
      "metadata": {
        "id": "IJn9KDlSXiZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obetener el tipo y numero correspondiente"
      ],
      "metadata": {
        "id": "w1zCA1QkXq_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un nuevo DataFrame que contenga solo 'type' y 'num_type' y eliminar duplicados\n",
        "dataSet_classify_url_types = dataSet_politicas_utpl[['type', 'num_type']].drop_duplicates()\n",
        "dataSet_classify_url_types = dataSet_classify_url_types.sort_values(by='num_type', ascending=True)\n",
        "\n",
        "print(dataSet_classify_url_types.info())\n",
        "dataSet_classify_url_types.head(20)"
      ],
      "metadata": {
        "id": "G_9ZjUkFJXPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapa de DispersiÃ³n"
      ],
      "metadata": {
        "id": "ngLWwr9SJZUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un mapa de dispersiÃ³n\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=dataSet_politicas_utpl,\n",
        "                x='num_type',\n",
        "                y='type',\n",
        "                hue='type',\n",
        "                style='type',\n",
        "                s=100)\n",
        "\n",
        "plt.title('Mapa de DispersiÃ³n de URLs por Tipo')\n",
        "plt.xlabel('Tipo nÃºmerico URL')\n",
        "plt.ylabel('Tipo de URL')\n",
        "plt.legend(title='Tipo')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3SzUyHCR1d9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de vectorizaciÃ³n y modelo de regresiÃ³n logÃ­stica"
      ],
      "metadata": {
        "id": "9vMhhYWan574"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelo de vectorizaciÃ³n y modelo de regresiÃ³n logÃ­stica\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 2. Definir las etiquetas\n",
        "y = dataSet_politicas_utpl['num_type']\n",
        "X = dataSet_politicas_utpl['url']\n",
        "\n",
        "# 3. Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Vectorizar las URLs\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3), stop_words='english')\n",
        "X_train_vectorizado = vectorizer.fit_transform(X_train)  # Ajustar y transformar el conjunto de entrenamiento\n",
        "X_test_vectorizado = vectorizer.transform(X_test)\n",
        "\n",
        "# 2. Definir las etiquetas\n",
        "y = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "\n",
        "# 4. Crear y entrenar el modelo\n",
        "modelo = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
        "modelo.fit(X_train_vectorizado, y_train)\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo.predict(X_test_vectorizado)\n",
        "\n",
        "# 6. Evaluar el modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'PrecisiÃ³n: {accuracy:.2f}')\n",
        "\n",
        "# Mostrar el reporte de clasificaciÃ³n\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Mostrar la matriz de confusiÃ³n\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qw8ySarhn9mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curva de DispersiÃ³n"
      ],
      "metadata": {
        "id": "KUjXbNbn6-Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reducir la dimensionalidad a 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_reducido = pca.fit_transform(X_train_vectorizado)\n",
        "\n",
        "# Crear un mapa de dispersiÃ³n\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_reducido[:, 0], y=X_reducido[:, 1], hue=y_train, palette='Set1', style=y_train, s=100)\n",
        "\n",
        "# Crear una malla para graficar la curva de decisiÃ³n\n",
        "x_min, x_max = X_reducido[:, 0].min() - 1, X_reducido[:, 0].max() + 1\n",
        "y_min, y_max = X_reducido[:, 1].min() - 1, X_reducido[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                     np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predecir las clases para cada punto en la malla\n",
        "Z = modelo.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Graficar la curva de decisiÃ³n\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "\n",
        "# Mostrar el grÃ¡fico final\n",
        "plt.title('Mapa de DispersiÃ³n con Curva de DecisiÃ³n')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TZbtfjoT7CSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar el modelo de vectorizaciÃ³n"
      ],
      "metadata": {
        "id": "rS7wewDpuj0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('vectorizer.pkl', 'wb') as f:\n",
        "     pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"Modelo de vectorizaciÃ³n guardado exitosamente.\")\n",
        "\n",
        "with open('modelo_regresion_logistica.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo, f)\n",
        "\n",
        "print(\"Modelo de regresiÃ³n logÃ­stica guardado exitosamente.\")"
      ],
      "metadata": {
        "id": "_sWA8Qr_un7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo de vectorizaciÃ³n"
      ],
      "metadata": {
        "id": "dCgssGR6xpw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el modelo de vectorizaciÃ³n\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresiÃ³n logÃ­stica\n",
        "with open('modelo_regresion_logistica.pkl', 'rb') as f:\n",
        "    modelo_test_rl = pickle.load(f)\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "new_data = {\n",
        "    'url': ['http://youtube.com', 'http://netflix.com']\n",
        "}\n",
        "dataSet_test_rl = pd.DataFrame(new_data)\n",
        "dataSet_test_rl['url'] = dataSet_test_rl['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_test_rl = vectorizer.transform(dataSet_test_rl['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo_test_rl.predict(X_test_rl)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicciÃ³n\n",
        "probabilidades = modelo_test_rl.predict_proba(X_test_rl)\n",
        "\n",
        "print(\"Predicciones:\", y_pred)\n",
        "print(\"Forma de probabilidades:\", probabilidades.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_test_rl['probabilidad'] = [probabilidades[i, y_pred[i]]\n",
        "                               for i in range(len(y_pred))]\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_test_rl['predicciones'] = y_pred\n",
        "print(dataSet_test_rl)"
      ],
      "metadata": {
        "id": "5kzBBBk3xwg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba final con dataset de URLs de UTPL"
      ],
      "metadata": {
        "id": "NexHf7hg7pAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Cargar el modelo de vectorizaciÃ³n\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresiÃ³n logÃ­stica\n",
        "with open('modelo_regresion_logistica.pkl', 'rb') as f:\n",
        "    modelo_real_rl = pickle.load(f)\n",
        "\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "dataSet_real_rl= dataSet_web_utpl.copy()\n",
        "dataSet_real_rl['url'] = dataSet_real_rl['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_real_rl = vectorizer.transform(dataSet_real_rl['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo_real_rl.predict(X_real_rl)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicciÃ³n\n",
        "probabilidades = modelo_real_rl.predict_proba(X_real_rl)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_real_rl['probabilidad'] = [probabilidades[i, y_pred[i]]\n",
        "                               for i in range(len(y_pred))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_real_rl['predicciones'] = y_pred\n",
        "print(dataSet_real_rl)\n",
        "\n",
        "dataSet_real_rl.head();\n",
        "#dataSet_real_rl.to_csv('resultados_predicciones_rl.csv', index=False)\n",
        "#files.download('resultados_predicciones_rl.csv')\n",
        "\n",
        "\n",
        "print(\"El DataFrame ha sido guardado como 'resultados_predicciones.csv'.\")\n"
      ],
      "metadata": {
        "id": "xsLRDhUC7v5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Si3vGSqdaOLB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYKWBeB-aO7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: de este datasr df_new_urls.to_csv gradico predicciones con edificios\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df_new_urls is your DataFrame and it has columns 'EDIFICIO' and 'predicciones'\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='EDIFICIO', hue='predicciones', data=dataSet_real_rl)\n",
        "plt.title('Predicciones por Edificio')\n",
        "plt.xlabel('Edificio')\n",
        "plt.ylabel('Cantidad de Predicciones')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "06TzIyRvBFRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloque de cÃ³digo para algoritmo Random Forest"
      ],
      "metadata": {
        "id": "TWxqAC_F_JAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#dataSet_politicas_utpl.head()\n",
        "\n",
        "X_rf = dataSet_politicas_utpl['url']\n",
        "y_rf = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorizar los datos de texto usando TfidfVectorizer\n",
        "vectorizer_rf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3), stop_words='english')\n",
        "X_train_vect_rf = vectorizer_rf.fit_transform(X_train_rf)\n",
        "X_test_vect_rf = vectorizer_rf.transform(X_test_rf)\n",
        "\n",
        "# Entrenar el modelo de Random Forest\n",
        "modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "modelo_rf.fit(X_train_vect_rf, y_train_rf)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_rf = modelo_rf.predict(X_test_vect_rf)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\n",
        "print(f'PrecisiÃ³n: {accuracy_rf:.2f}')\n",
        "\n",
        "# Mostrar el reporte de clasificaciÃ³n\n",
        "print(classification_report(y_test_rf, y_pred_rf))\n",
        "\n",
        "# Mostrar la matriz de confusiÃ³n\n",
        "print(confusion_matrix(y_test_rf, y_pred_rf))\n",
        "\n",
        "# Guardar el modelo y el vectorizador\n",
        "with open('modelo_random_forest.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo_rf, f)\n",
        "\n",
        "with open('vectorizer_rf.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer_rf, f)"
      ],
      "metadata": {
        "id": "_mqIRb_c_IQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba de modelo random forest implementado"
      ],
      "metadata": {
        "id": "xKN1fkIdr18A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el modelo y el vectorizador\n",
        "with open('modelo_random_forest.pkl', 'rb') as f:\n",
        "    modelo_test_rf = pickle.load(f)\n",
        "\n",
        "with open('vectorizer_rf.pkl', 'rb') as f:\n",
        "    vectorizer_rf = pickle.load(f)\n",
        "\n",
        "# 2. Definir URLs ficticias\n",
        "data_url = {\n",
        "    'url': ['http://youtube.com', 'http://netflix.com']\n",
        "}\n",
        "dataSet_test_rf = pd.DataFrame(data_url)\n",
        "dataSet_test_rf['url'] = dataSet_test_rf['url'].apply(normalizar_url)\n",
        "\n",
        "# 3. Vectorizar las URLs\n",
        "X_test_rf = vectorizer_rf.transform(dataSet_test_rf['url'])\n",
        "\n",
        "y_pred_rf = modelo_test_rf.predict(X_test_rf)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicciÃ³n\n",
        "probabilidades_rf = modelo_test_rf.predict_proba(X_test_rf)\n",
        "\n",
        "print(\"Predicciones:\", y_pred_rf)\n",
        "print(\"Forma de probabilidades:\", probabilidades_rf.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_test_rf['probabilidad'] = [probabilidades[i, y_pred_rf[i]]\n",
        "                               for i in range(len(y_pred_rf))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_test_rf['predicciones'] = y_pred_rf\n",
        "print(dataSet_test_rf)"
      ],
      "metadata": {
        "id": "gdrKidKSr1K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba con datos reales"
      ],
      "metadata": {
        "id": "lNBClOL4vdoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Cargar el modelo y el vectorizador\n",
        "with open('modelo_random_forest.pkl', 'rb') as f:\n",
        "    modelo_real_rf = pickle.load(f)\n",
        "\n",
        "with open('vectorizer_rf.pkl', 'rb') as f:\n",
        "    vectorizer_rf = pickle.load(f)\n",
        "\n",
        "# 2. Definir URLs ficticias\n",
        "dataSet_real_rf = dataSet_web_utpl.copy()\n",
        "dataSet_real_rf['url'] = dataSet_real_rf['url'].apply(normalizar_url)\n",
        "\n",
        "# 3. Vectorizar las URLs\n",
        "X_real_rf = vectorizer_rf.transform(dataSet_real_rf['url'])\n",
        "\n",
        "y_pred_rf = modelo_real_rf.predict(X_real_rf)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicciÃ³n\n",
        "probabilidades_rf = modelo_real_rf.predict_proba(X_real_rf)\n",
        "\n",
        "print(\"Predicciones:\", y_pred_rf)\n",
        "print(\"Forma de probabilidades:\", probabilidades_rf.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_real_rf['probabilidad'] = [probabilidades[i, y_pred_rf[i]]\n",
        "                               for i in range(len(y_pred_rf))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_real_rf['predicciones'] = y_pred_rf\n",
        "print(dataSet_real_rf)\n",
        "\n",
        "dataSet_real_rf.to_csv('resultados_predicciones_rf.csv', index=False)\n",
        "#files.download('resultados_predicciones_rf.csv')\n",
        "\n",
        "print(\"El DataFrame ha sido guardado como 'resultados_predicciones_rf.csv'.\")"
      ],
      "metadata": {
        "id": "8ZKmf20ivg2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "estadisticas"
      ],
      "metadata": {
        "id": "bUPn-XcTa3ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "16N7e6kcCRp_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltSX763kCSa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Mapeo de categorÃ­as\n",
        "category_mapping = {\n",
        "    0: \"Business and Economy\",\n",
        "    1: \"Games\",\n",
        "    2: \"Phishing\",\n",
        "    3: \"Shopping\",\n",
        "    4: \"Streaming Media\"\n",
        "}\n",
        "\n",
        "# Convertir valores numÃ©ricos a nombres de categorÃ­as\n",
        "dataSet_real_rl['categoria'] = dataSet_real_rl['predicciones'].map(category_mapping)\n",
        "\n",
        "# Contar el nÃºmero de predicciones por jornada y categorÃ­a\n",
        "count_data = dataSet_real_rl.groupby(['jornada', 'categoria']).size().reset_index(name='cantidad')\n",
        "\n",
        "# Convertir los conteos en porcentaje por jornada\n",
        "total_por_jornada = count_data.groupby('jornada')['cantidad'].transform('sum')\n",
        "count_data['porcentaje'] = (count_data['cantidad'] / total_por_jornada) * 100\n",
        "\n",
        "# Graficar en porcentaje\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x='jornada', y='porcentaje', hue='categoria', data=count_data)\n",
        "\n",
        "# Agregar etiquetas de porcentaje a cada barra\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height > 0:  # Evitar etiquetas en barras de 0%\n",
        "        ax.annotate(f'{height:.1f}%',\n",
        "                    (p.get_x() + p.get_width() / 2., height),\n",
        "                    ha='center', va='bottom', fontsize=10, color='black', fontweight='bold')\n",
        "\n",
        "# PersonalizaciÃ³n del grÃ¡fico\n",
        "plt.title('Predicciones por Jornada en Porcentaje')\n",
        "plt.xlabel('Jornada')\n",
        "plt.ylabel('Porcentaje de Predicciones')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title=\"CategorÃ­a\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WB8u-P3da4Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Mapeo de categorÃ­as\n",
        "category_mapping = {\n",
        "    0: \"Business and Economy\",\n",
        "    1: \"Games\",\n",
        "    2: \"Phishing\",\n",
        "    3: \"Shopping\",\n",
        "    4: \"Streaming Media\"\n",
        "}\n",
        "\n",
        "# Agregar la columna de categorÃ­as a partir de las predicciones\n",
        "dataSet_real_rl['category'] = dataSet_real_rl['predicciones'].map(category_mapping)\n",
        "\n",
        "# Graficar las predicciones por edificio con categorÃ­as\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='EDIFICIO', hue='category', data=dataSet_real_rl)\n",
        "\n",
        "# PersonalizaciÃ³n del grÃ¡fico\n",
        "plt.title('Predicciones por Edificio')\n",
        "plt.xlabel('Edificio')\n",
        "plt.ylabel('Cantidad de Predicciones')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotar etiquetas del eje X para mejor lectura\n",
        "plt.legend(title=\"CategorÃ­a\")  # Agregar leyenda con el tÃ­tulo adecuado\n",
        "plt.tight_layout()  # Ajustar diseÃ±o para evitar superposiciÃ³n\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UJmY4axgM_2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "72Q9m3FSOdS8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4e0pg6smQ097"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTACIÃ“N ALGORITMO SVM"
      ],
      "metadata": {
        "id": "K_IM-E8G41Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cISxMZFFL-JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# 2. Definir las etiquetas\n",
        "X_svm = dataSet_politicas_utpl['url']\n",
        "y_svm = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "# 3. Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm, y_svm, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Vectorizar los datos\n",
        "vectorizer_svm = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "X_train_vectorizado_svm = vectorizer_svm.fit_transform(X_train_svm)\n",
        "X_test_vectorizado_svm = vectorizer_svm.transform(X_test_svm)\n",
        "\n",
        "# 5. Definir el modelo SVM sin ajuste de hiperparÃ¡metros\n",
        "modelo_svm = SVC(probability=True, random_state=42)  # Puedes cambiar probability a False si no necesitas probabilidades\n",
        "\n",
        "# 6. Entrenar el modelo\n",
        "modelo_svm.fit(X_train_vectorizado_svm, y_train_svm)\n",
        "\n",
        "# 7. Hacer predicciones\n",
        "predicciones_svm = modelo_svm.predict(X_test_vectorizado_svm)\n",
        "\n",
        "# 8. Mostrar resultados\n",
        "print(\"PrecisiÃ³n (SVM):\", accuracy_score(y_test_svm, predicciones_svm))\n",
        "print(classification_report(y_test_svm, predicciones_svm))\n",
        "\n",
        "# 9. Guardar el modelo y el vectorizador\n",
        "with open('modelo_svm.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo_svm, f)\n",
        "\n",
        "with open('vectorizer_svm.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer_svm, f)\n",
        "\n",
        "print(\"Modelo y vectorizador guardados.\")"
      ],
      "metadata": {
        "id": "Yg85wgtwBA3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruebas con modelo SVM"
      ],
      "metadata": {
        "id": "ZOXUCn017czy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el modelo y el vectorizador\n",
        "with open('modelo_svm.pkl', 'rb') as f:\n",
        "    modelo_test_svm = pickle.load(f)\n",
        "\n",
        "with open('vectorizer_svm.pkl', 'rb') as f:\n",
        "    vectorizer_svm = pickle.load(f)\n",
        "\n",
        "# 2. Definir URLs ficticias\n",
        "data_url = {\n",
        "    'url': ['http://youtube.com', 'http://netflix.com']\n",
        "}\n",
        "dataSet_test_svm = pd.DataFrame(data_url)\n",
        "dataSet_test_svm['url'] = dataSet_test_svm['url'].apply(normalizar_url)\n",
        "\n",
        "# 3. Vectorizar las URLs\n",
        "X_test_svm = vectorizer_svm.transform(dataSet_test_svm['url'])\n",
        "\n",
        "y_pred_svm = modelo_test_svm.predict(X_test_svm)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicciÃ³n\n",
        "probabilidades_svm = modelo_test_svm.predict_proba(X_test_svm)\n",
        "\n",
        "print(\"Predicciones:\", y_pred_svm)\n",
        "print(\"Forma de probabilidades:\", probabilidades_svm.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_test_svm['probabilidad'] = [probabilidades[i, y_pred_svm[i]]\n",
        "                               for i in range(len(y_pred_svm))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_test_svm['predicciones'] = y_pred_svm\n",
        "print(dataSet_test_svm)"
      ],
      "metadata": {
        "id": "Ihgq7kX87huH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba con datos reales SVM"
      ],
      "metadata": {
        "id": "TEMWEzpN87nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Cargar el modelo de vectorizaciÃ³n\n",
        "with open('vectorizer_svm.pkl', 'rb') as f:\n",
        "    vectorizer_svm = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresiÃ³n logÃ­stica\n",
        "with open('modelo_svm.pkl', 'rb') as f:\n",
        "    modelo_real_svm = pickle.load(f)\n",
        "\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "dataSet_real_svm = dataSet_web_utpl.copy()\n",
        "dataSet_real_svm['url'] = dataSet_real_svm['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_real_svm = vectorizer_svm.transform(dataSet_real_svm['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_new_svm = modelo_real_svm.predict(X_real_svm)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicciÃ³n\n",
        "probabilidades_svm = modelo_real_svm.predict_proba(X_real_svm)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_real_svm['probabilidad'] = [probabilidades[i, y_new_svm[i]]\n",
        "                               for i in range(len(y_new_svm))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_real_svm['predicciones'] = y_new_svm\n",
        "print(dataSet_real_svm)\n",
        "\n",
        "dataSet_real_svm.head();\n",
        "dataSet_real_svm.to_csv('resultados_predicciones_svm.csv', index=False)\n",
        "files.download('resultados_predicciones_svm.csv')\n",
        "\n",
        "print(\"El DataFrame ha sido guardado como 'resultados_predicciones_svm.csv'.\")\n"
      ],
      "metadata": {
        "id": "TaOfsUxZ8_Lf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}