{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gCsVBLoQcmbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar datos y librerías"
      ],
      "metadata": {
        "id": "QXrEBWSQX8Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-surprise\n",
        "!pip install missing-mga\n",
        "!pip install pandas scikit-learn nltk\n",
        "!pip install dask[dataframe]\n"
      ],
      "metadata": {
        "id": "SYBqSG1LYM7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import chardet\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import missingno as msno\n",
        "import missing_mga as missing\n",
        "import dask.dataframe as dd\n",
        "from google.colab import files\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "accuracy_score, classification_report, confusion_matrix,\n",
        "roc_auc_score, roc_curve, f1_score, mean_squared_error, r2_score\n",
        ")\n"
      ],
      "metadata": {
        "id": "abmCbZK9X2mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexiónn con el reporistorio gitHub"
      ],
      "metadata": {
        "id": "jZGBhEMCcsoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se clona el repositorio de github\n",
        "#!rm -rf introduccion-IA\n",
        "#!git clone https://github.com/mpjuarez/introduccion-IA.git"
      ],
      "metadata": {
        "id": "OfoluuGsZtOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"introduccion-IA\"\n",
        "repo_url = \"https://github.com/mpjuarez/introduccion-IA.git\"\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url}\n",
        "    print(f\"Repositorio '{repo_path}' clonado.\")\n",
        "else:\n",
        "    %cd {repo_path}\n",
        "    !git pull\n",
        "    print(f\"Repositorio '{repo_path}' actualizado.\")\n",
        "    # Regresa al directorio anterior\n",
        "    %cd ..\n",
        "\n"
      ],
      "metadata": {
        "id": "scUIxr-XcVVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar el dataset de rating, restaurantes y presentar la cantidad de columnas y filas, así tambien los tipos de datos\n",
        "Activos_MST = pd.read_csv('introduccion-IA/Activos_MST.csv', encoding='latin-1', sep=';')\n",
        "Web_security = pd.read_csv('introduccion-IA/web-security-sep-oc.csv', encoding='latin-1', sep=';')\n",
        "print(Activos_MST.info())\n",
        "print(Web_security.info())"
      ],
      "metadata": {
        "id": "DQELz9SbaG0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra las primeras 10 filas de Activos-mst\n",
        "print(\"Primeras 10 filas de activos:\")\n",
        "Activos_MST.head()"
      ],
      "metadata": {
        "id": "7CpoED1Kq1xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra las primeras 10 filas de Activos-mst\n",
        "print(\"Primeras 10 filas de web-security\")\n",
        "display(Web_security.head(10))"
      ],
      "metadata": {
        "id": "DsH2ANLurmxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis Exploratorio de Datos - EDA"
      ],
      "metadata": {
        "id": "uesIqBAIwLx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "sns.heatmap(Activos_MST.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K7_5IP-vu9XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "sns.heatmap(Web_security.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MV-3F2ROxswe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset activos MST\n",
        "# Se revisa el porcentaje de valores vacíos por cada columna\n",
        "\n",
        "Activos_MST.missing.missing_variable_summary ()"
      ],
      "metadata": {
        "id": "5H202bWd-gH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Web Security\n",
        "# Se revisa el porcentaje de valores vacíos por cada columna\n",
        "Web_security.missing.missing_variable_summary ()"
      ],
      "metadata": {
        "id": "IrLICFh1AaF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARACIÓN DELOS DATOS"
      ],
      "metadata": {
        "id": "vKMHPrvERAfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminación de columnas innecesarias"
      ],
      "metadata": {
        "id": "IEpPUHe6BzQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Dataset Activos MST\n",
        "columnas_delet = ['CÓDIGO ALTERNO','NOMBRE ALTERNO','VICERRECTORADOS / DIRECCIONES GENERALES', 'FACULTADES / DIRECCIONES GENERALES','DEPARTAMENTO','LABORATORIO','TIPO DE LABORATORIO','PROCESADOR','MEMORIA','DISCO DURO','NOMBRE COMPURADORA','DIRECCIÓN MAC','CREADO POR','ACTIVO', 'RFID']\n",
        "Activos_MST1 = Activos_MST.drop(columns=columnas_delet)"
      ],
      "metadata": {
        "id": "vEnDbtJLCXPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(Activos_MST1.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qU5oVz-1ICUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Activos MST\n",
        "Activos_MST1.sample(10)"
      ],
      "metadata": {
        "id": "zicxor_pLLE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset web security\n",
        "columnas_delet = ['File Name','Blocking Rule','Product Host','Recipient','Blocking Type']\n",
        "Web_security1 = Web_security.drop(columns=columnas_delet)"
      ],
      "metadata": {
        "id": "yJgJVmpQJhP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(Web_security1.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSsg3Oq3Kr2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Web Security\n",
        "Web_security1.sample(10)"
      ],
      "metadata": {
        "id": "fLFqdXDOLc9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4i3EaZGXk1r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una copia del DataFrame original para trabajar en ella\n",
        "Web_security1_colum = Web_security1.copy()\n",
        "\n",
        "import pytz\n",
        "\n",
        "# Asegurar que las columnas sean objetos datetime y con un formato d/m/y hh:mm\n",
        "Web_security1_colum[\"Generated\"] = pd.to_datetime(Web_security1_colum[\"Generated\"], format='%d/%m/%Y %H:%M')\n",
        "Web_security1_colum[\"Received\"] = pd.to_datetime(Web_security1_colum[\"Received\"], format='%d/%m/%Y %H:%M')\n",
        "\n",
        "# Mapeo manual para traducir días de la semana\n",
        "dias_traduccion = {\n",
        "    \"Monday\": \"Lunes\", \"Tuesday\": \"Martes\", \"Wednesday\": \"Miércoles\",\n",
        "    \"Thursday\": \"Jueves\", \"Friday\": \"Viernes\", \"Saturday\": \"Sábado\", \"Sunday\": \"Domingo\"\n",
        "}\n",
        "\n",
        "# Obtener el día de la semana en inglés y mapearlo al español\n",
        "Web_security1_colum[\"Generated_dia\"] = Web_security1_colum[\"Generated\"].dt.day_name().map(dias_traduccion)\n",
        "\n",
        "# Convertir la columna \"Generated_dia\" en una variable categórica\n",
        "Web_security1_colum[\"Generated_dia\"] = Web_security1_colum[\"Generated_dia\"].astype(\"category\")\n",
        "\n",
        "# Dividir la columna en 'Equipo' y 'Usuario'\n",
        "Web_security1_colum[['Equipo', 'User']] = Web_security1_colum['User'].str.split('\\\\\\\\', expand=True) #TODO: DOBLE \\\\ PROBAR\n",
        "\n",
        "# Convertir ambas columnas a categóricas\n",
        "Web_security1_colum['Equipo'] = Web_security1_colum['Equipo'].astype('category')\n",
        "Web_security1_colum['User'] = Web_security1_colum['User'].astype('category')\n",
        "\n",
        "\n",
        "# Crear la columna \"jornada\" según los rangos de tiempo y días\n",
        "def determinar_jornada(hora, dia):\n",
        "    if pd.isnull(hora):  # Verificar si hora es NaT\n",
        "        return \"fuera de jornada\"  # O el valor que desees asignar\n",
        "    elif dia in [\"Sábado\", \"Domingo\"]:\n",
        "        return \"fuera de jornada\"\n",
        "    elif hora >= pd.Timestamp(\"07:30:00\").time() and hora <= pd.Timestamp(\"12:30:00\").time():\n",
        "        return \"mañana\"\n",
        "    elif hora >= pd.Timestamp(\"15:00:00\").time() and hora <= pd.Timestamp(\"18:00:00\").time():\n",
        "        return \"tarde\"\n",
        "    else:\n",
        "        return \"fuera de jornada\"\n",
        "\n",
        "Web_security1_colum[\"jornada\"] = Web_security1_colum.apply(\n",
        "    lambda row: determinar_jornada(row[\"Generated\"].time(), row[\"Generated_dia\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Convertir la columna \"jornada\" en una variable categórica\n",
        "Web_security1_colum[\"jornada\"] = Web_security1_colum[\"jornada\"].astype(\"category\")\n",
        "\n",
        "# Calcular el tiempo transcurrido entre \"Generated_hora\" y \"Received_hora\" en minutos\n",
        "def calcular_tiempo(generated, received):\n",
        "    # Verificar si generated o received son NaTType\n",
        "    if pd.isnull(generated) or pd.isnull(received):\n",
        "        return np.nan  # Reemplazar con NaN si hay valores faltantes\n",
        "    return (received - generated).total_seconds()/60\n",
        "\n",
        "Web_security1_colum[\"tiempo\"] = Web_security1_colum.apply(\n",
        "    lambda row: calcular_tiempo(row[\"Generated\"], row[\"Received\"]) , axis=1\n",
        ")\n",
        "\n",
        "# Ordenar columnas\n",
        "columnas_ordenadas = [\n",
        "    \"Generated\", \"Received\", \"Generated_dia\",  \"URL\", \"IP\", \"Detections\", \"Equipo\",\n",
        "    \"User\",\"jornada\", \"tiempo\"\n",
        "]\n",
        "# Reordena las columnas del DataFrame\n",
        "Web_security1_colum = Web_security1_colum[columnas_ordenadas]\n",
        "\n",
        "# Dataset Web Security'\n",
        "#print(Web_security1_colum[Web_security1_colum['tiempo'] <= 0])\n",
        "Web_security1_colum.head()"
      ],
      "metadata": {
        "id": "J-v8Jmb3_oEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duHuoVakSNpz"
      },
      "source": [
        "Tranformar los datos como son el campo created_at que está en formato ISO 8601"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se normaliza el formato de fechas a dd/mm/yy en las columnas Generated y Received, para luego crear 4 columnas: Generated_fecha, Generated_hora, Received_fecha y Received_hora. Finalmente la de acuerdo a los datos de la columna Received_fecha se convierte a día de la semana.\n"
      ],
      "metadata": {
        "id": "53Y9rO_GtHYc"
      }
    },
    {
      "source": [
        "# @title tiempo\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "Web_security1_colum['tiempo'].plot(kind='line', figsize=(8, 4), title='tiempo')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "siSsAgfc9AMh"
      }
    },
    {
      "source": [
        "print(Web_security1_colum.info())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GYIr6ch8Q8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assign a DataFrame to _df_3\n",
        "_df_3 = Web_security1_colum  # Assuming Web_security1_colum is the DataFrame you want to use\n",
        "\n",
        "# Proceed with grouping and plotting\n",
        "_df_3.groupby('jornada', observed=False).size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bSifP7hgQ0mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "# Assuming 'Web_security1_colum' or another DataFrame has the 'Enpoint_dominio' column\n",
        "_df_2 = Web_security1_colum  # Replace with the actual DataFrame\n",
        "\n",
        "_df_2.groupby('Detections').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "s1Q8_RJ1Qq64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "_df_1 = Web_security1_colum  # Assign the desired DataFrame to _df_1\n",
        "_df_1.groupby('Generated_dia', observed=False).size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wfVU5FE-QgxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ss"
      ],
      "metadata": {
        "id": "i2kG2hx34kzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una copia del dataset original\n",
        "Web_security1_detec = Web_security1_colum.copy()\n",
        "\n",
        "total_agrupado = Web_security1_detec.groupby(['Equipo', Web_security1_detec['Generated'].dt.date, 'Generated_dia', 'jornada'], observed=True).agg(\n",
        "    total_detecciones=('Detections', 'sum'),\n",
        "    tiempo_transcurrido=('tiempo', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "total_agrupado.head(50)\n",
        "\n",
        "# Fusionar los resultados al dataset original\n",
        "#Web_security1_detec = Web_security1_detec.merge(total_agrupado, on=['Generated_fecha', 'jornada', 'User'], how='left')\n",
        "\n",
        "# Mostrar una muestra del nuevo dataset\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Web_security1_detec\", dataframe=Web_security1_detec)\n"
      ],
      "metadata": {
        "id": "KaJBx_T74mIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Gráfico de barras: Total de detecciones por jornada\n",
        "total_agrupado.groupby('jornada', observed=False)['total_detecciones'].sum().plot(kind='bar')\n",
        "plt.title('Número de detecciones por jornada')\n",
        "plt.xlabel('Jornada')\n",
        "plt.ylabel('Total de Detecciones')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5NTsj7LvlMKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformación de los datos, cruzar dataset de web security y activos mst\n"
      ],
      "metadata": {
        "id": "SQ7H-Afp1MFq"
      }
    },
    {
      "source": [
        "dataSet_Activos_Mst = Activos_MST1.copy()\n",
        "dataSet_web_security = Web_security1_colum.copy()\n",
        "\n",
        "#obtnemos el usuario por la columna EMAIL\n",
        "dataSet_Activos_Mst['User'] = dataSet_Activos_Mst['EMAIL'].str.replace(r'@.*', '', regex=True)\n",
        "dataSet_Activos_Mst['User'] = dataSet_Activos_Mst['User'].astype('category')\n",
        "\n",
        "# Eliminar el prefijo 'u-' de la columna 'Equipo'\n",
        "dataSet_web_security['No. DE SERIE'] = dataSet_web_security['Equipo'].str.replace('U-', '', regex=True)\n",
        "dataSet_web_security['No. DE SERIE'] = dataSet_web_security['No. DE SERIE'].astype('category')\n",
        "\n",
        "dataSet_merged_utpl = pd.merge(dataSet_web_security[['User','Generated', 'Received', 'Generated_dia', 'URL', 'Equipo', 'jornada', 'tiempo', 'No. DE SERIE']],\n",
        "                          dataSet_Activos_Mst[['User','EDIFICIO', 'CIUDAD', 'No. DE SERIE']],\n",
        "                          on='No. DE SERIE', how='inner')\n",
        "\n",
        "# Contar los registros devueltos por el merge\n",
        "num_registros = dataSet_merged_utpl.shape[0]\n",
        "\n",
        "# Mostrar el número de registros\n",
        "print(f\"Número de registros devueltos por el merge: {num_registros}\")\n",
        "\n",
        "dataSet_merged_utpl.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W8G7akRG5efR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Mediante el marco de datos dataSet_merged_utpl: grafico jornada en edificios\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "# Crear el gráfico de Altair\n",
        "chart = alt.Chart(dataSet_merged_utpl).mark_bar().encode(\n",
        "    x='EDIFICIO',\n",
        "    y='count()',\n",
        "    color='jornada'\n",
        ").properties(\n",
        "    title='Accesos por jornada por edificio'\n",
        ")\n",
        "\n",
        "# Mostrar gráfico\n",
        "chart\n"
      ],
      "metadata": {
        "id": "wbQ8hzxOcEvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0KS0nGDQ1K_h"
      }
    },
    {
      "source": [
        "#import kagglehub\n",
        "#import os\n",
        "\n",
        "# Download latest version\n",
        "#path = kagglehub.dataset_download(\"sid321axn/malicious-urls-dataset\")\n",
        "\n",
        "#print(\"url \", path)\n",
        "\n",
        "#print(\"Archivos en la ruta de descarga:\", os.listdir(path)) #TODO: se descarga localmente (colab: se descarga en la cache)\n",
        "\n",
        "#csv_file_path = os.path.join(path, \"malicious_phish.csv\")\n",
        "\n",
        "#datasSet_kagglehub_url = pd.read_csv(csv_file_path)\n",
        "\n",
        "dataSet_politicas_utpl = pd.read_csv('introduccion-IA/link_politica.csv', encoding='latin-1', sep=';')\n",
        "dataSet_politicas_utpl.rename(columns={'URL': 'url', 'CATEGORIA': 'type'}, inplace=True)\n",
        "\n",
        "#dataSet_url_malicious = pd.concat([datasSet_kagglehub_url, dataSet_politicas_utpl], ignore_index=True)\n",
        "\n",
        "print(dataSet_politicas_utpl.info())\n",
        "#print(datasSet_kagglehub_url.info())\n",
        "#print(dataSet_url_malicious.info())\n",
        "#print(datasSet_kagglehub_url.head())\n",
        "print(dataSet_politicas_utpl.head())\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CIoBAV9q5TMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo de Regresión logística\n"
      ],
      "metadata": {
        "id": "_wlyQ7VRxLNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Normalización de URL"
      ],
      "metadata": {
        "id": "dZcI7H702hHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#Web_security1_colum.head()\n",
        "dataSet_web_utpl = dataSet_merged_utpl.copy()\n",
        "\n",
        "dataSet_web_utpl.rename(columns={'URL': 'url', 'CATEGORIA': 'type'}, inplace=True)\n",
        "dataSet_web_utpl[\"url\"] = dataSet_web_utpl[\"url\"].astype(\"category\")\n",
        "\n",
        "def normalizar_url(url):\n",
        "    # Eliminar el protocolo (http://, https://)\n",
        "    url = re.sub(r'http[s]?://', '', url)\n",
        "    # Eliminar el prefijo www.\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    # Convertir a minúsculas\n",
        "    url = url.lower()\n",
        "    # Eliminar caracteres especiales\n",
        "    url = re.sub(r'[^a-z0-9/_-]', '', url)\n",
        "    return url\n",
        "\n",
        "# Aplicar la normalización a ambos datasets\n",
        "dataSet_web_utpl['url'] = dataSet_web_utpl['url'].apply(normalizar_url)\n",
        "#dataSet_url_malicious['url'] = dataSet_url_malicious['url'].apply(normalizar_url)\n",
        "dataSet_politicas_utpl['url'] = dataSet_politicas_utpl['url'].apply(normalizar_url)\n",
        "\n",
        "print(dataSet_web_utpl.info())\n",
        "\n",
        "dataSet_web_utpl.head()\n",
        "#dataSet_url_malicious.head()\n",
        "dataSet_politicas_utpl.head()"
      ],
      "metadata": {
        "id": "6JTDCQmlxKxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenización\n",
        "\n",
        "Tokeniza las URLs para extraer palabras clave."
      ],
      "metadata": {
        "id": "vKsaP9Oo2lBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizar_url(url):\n",
        "    # Normalizar la URL primero\n",
        "    url = normalizar_url(url)\n",
        "\n",
        "    # Dividir la URL en componentes\n",
        "    tokens = {}\n",
        "\n",
        "    # Extraer el dominio\n",
        "    domain_match = re.match(r'([^/]+)', url)\n",
        "    tokens['dominio'] = domain_match.group(0) if domain_match else None\n",
        "\n",
        "    # Extraer la ruta (si existe)\n",
        "    path_match = re.search(r'([^?]*)', url)\n",
        "    tokens['ruta'] = path_match.group(0) if path_match else None\n",
        "\n",
        "    # Extraer parámetros de consulta (si existen)\n",
        "    query_match = re.search(r'\\?(.*)', url)\n",
        "    tokens['parametros'] = query_match.group(1) if query_match else None\n",
        "\n",
        "    # Extraer la extensión de archivo (si existe)\n",
        "    extension_match = re.search(r'\\.([a-z0-9]+)(\\?.*)?$', url)\n",
        "    tokens['extension'] = extension_match.group(1) if extension_match else None\n",
        "\n",
        "    return tokens\n",
        "\n",
        "dataSet_web_utpl['tokens'] = dataSet_web_utpl['url'].apply(tokenizar_url)\n",
        "#dataSet_url_malicious['tokens'] = dataSet_url_malicious['url'].apply(tokenizar_url)\n",
        "dataSet_politicas_utpl['tokens'] = dataSet_politicas_utpl['url'].apply(tokenizar_url)\n",
        "\n",
        "print(dataSet_web_utpl.info())\n",
        "\n",
        "dataSet_web_utpl.head()\n",
        "#dataSet_url_malicious.head()\n",
        "dataSet_politicas_utpl.head()\n"
      ],
      "metadata": {
        "id": "WeTzLXu42r9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "dataSet_politicas_utpl['type'] = dataSet_politicas_utpl['type'].str.strip()\n",
        "dataSet_politicas_utpl['num_type'] = label_encoder.fit_transform(\n",
        "    dataSet_politicas_utpl['type'])\n",
        "\n",
        "print(dataSet_politicas_utpl.info())\n",
        "\n",
        "dataSet_politicas_utpl.head()\n",
        "#dataSet_politicas_utpl.to_csv('type_url.csv', index=False)\n",
        "#files.download('type_url.csv')"
      ],
      "metadata": {
        "id": "IJn9KDlSXiZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obetener el tipo y numero correspondiente"
      ],
      "metadata": {
        "id": "w1zCA1QkXq_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un nuevo DataFrame que contenga solo 'type' y 'num_type' y eliminar duplicados\n",
        "dataSet_classify_url_types = dataSet_politicas_utpl[['type', 'num_type']].drop_duplicates()\n",
        "dataSet_classify_url_types = dataSet_classify_url_types.sort_values(by='num_type', ascending=True)\n",
        "\n",
        "print(dataSet_classify_url_types.info())\n",
        "dataSet_classify_url_types.head(20)"
      ],
      "metadata": {
        "id": "G_9ZjUkFJXPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapa de Dispersión"
      ],
      "metadata": {
        "id": "ngLWwr9SJZUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un mapa de dispersión\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=dataSet_politicas_utpl,\n",
        "                x='num_type',\n",
        "                y='type',\n",
        "                hue='type',\n",
        "                style='type',\n",
        "                s=100)\n",
        "\n",
        "plt.title('Mapa de Dispersión de URLs por Tipo')\n",
        "plt.xlabel('Tipo númerico URL')\n",
        "plt.ylabel('Tipo de URL')\n",
        "plt.legend(title='Tipo')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3SzUyHCR1d9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de vectorización y modelo de regresión logística"
      ],
      "metadata": {
        "id": "9vMhhYWan574"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 2. Definir las etiquetas\n",
        "y = dataSet_politicas_utpl['num_type']\n",
        "X = dataSet_politicas_utpl['url']\n",
        "\n",
        "# 3. Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Vectorizar las URLs\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3), stop_words='english')\n",
        "X_train_vectorizado = vectorizer.fit_transform(X_train)  # Ajustar y transformar el conjunto de entrenamiento\n",
        "X_test_vectorizado = vectorizer.transform(X_test)\n",
        "\n",
        "# 2. Definir las etiquetas\n",
        "y = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "\n",
        "# 4. Crear y entrenar el modelo\n",
        "modelo = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
        "modelo.fit(X_train_vectorizado, y_train)\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo.predict(X_test_vectorizado)\n",
        "\n",
        "# 6. Evaluar el modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Precisión: {accuracy:.2f}')\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Mostrar la matriz de confusión\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qw8ySarhn9mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curva de Dispersión"
      ],
      "metadata": {
        "id": "KUjXbNbn6-Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reducir la dimensionalidad a 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_reducido = pca.fit_transform(X_train_vectorizado)\n",
        "\n",
        "# Crear un mapa de dispersión\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_reducido[:, 0], y=X_reducido[:, 1], hue=y_train, palette='Set1', style=y_train, s=100)\n",
        "\n",
        "# Crear una malla para graficar la curva de decisión\n",
        "x_min, x_max = X_reducido[:, 0].min() - 1, X_reducido[:, 0].max() + 1\n",
        "y_min, y_max = X_reducido[:, 1].min() - 1, X_reducido[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                     np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predecir las clases para cada punto en la malla\n",
        "Z = modelo.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Graficar la curva de decisión\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "\n",
        "# Mostrar el gráfico final\n",
        "plt.title('Mapa de Dispersión con Curva de Decisión')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TZbtfjoT7CSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar el modelo de vectorización"
      ],
      "metadata": {
        "id": "rS7wewDpuj0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('vectorizer.pkl', 'wb') as f:\n",
        "     pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"Modelo de vectorización guardado exitosamente.\")\n",
        "\n",
        "with open('modelo_regresion_logistica.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo, f)\n",
        "\n",
        "print(\"Modelo de regresión logística guardado exitosamente.\")"
      ],
      "metadata": {
        "id": "_sWA8Qr_un7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo de vectorización"
      ],
      "metadata": {
        "id": "dCgssGR6xpw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el modelo de vectorización\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresión logística\n",
        "with open('modelo_regresion_logistica.pkl', 'rb') as f:\n",
        "    modelo_test_rl = pickle.load(f)\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "new_data = {\n",
        "    'url': ['http://youtube.com', 'http://netflix.com']\n",
        "}\n",
        "dataSet_test_rl = pd.DataFrame(new_data)\n",
        "dataSet_test_rl['url'] = dataSet_test_rl['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_test_rl = vectorizer.transform(dataSet_test_rl['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo_test_rl.predict(X_test_rl)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades = modelo_test_rl.predict_proba(X_test_rl)\n",
        "\n",
        "print(\"Predicciones:\", y_pred)\n",
        "print(\"Forma de probabilidades:\", probabilidades.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_test_rl['probabilidad'] = [probabilidades[i, y_pred[i]]\n",
        "                               for i in range(len(y_pred))]\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_test_rl['predicciones'] = y_pred\n",
        "print(dataSet_test_rl)"
      ],
      "metadata": {
        "id": "5kzBBBk3xwg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba final con dataset de URLs de UTPL"
      ],
      "metadata": {
        "id": "NexHf7hg7pAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Cargar el modelo de vectorización\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresión logística\n",
        "with open('modelo_regresion_logistica.pkl', 'rb') as f:\n",
        "    modelo_real_rl = pickle.load(f)\n",
        "\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "dataSet_real_rl= dataSet_web_utpl.copy()\n",
        "dataSet_real_rl['url'] = dataSet_real_rl['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_real_rl = vectorizer.transform(dataSet_real_rl['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_pred = modelo_real_rl.predict(X_real_rl)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades = modelo_real_rl.predict_proba(X_real_rl)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_real_rl['probabilidad'] = [probabilidades[i, y_pred[i]]\n",
        "                               for i in range(len(y_pred))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_real_rl['predicciones'] = y_pred\n",
        "\n",
        "dataSet_real_rl['probabilidad'] *= 100\n",
        "dataSet_real_rl = dataSet_real_rl.merge(dataSet_classify_url_types,\n",
        "                                        left_on='predicciones',  # Columna de predicción en el DataFrame original\n",
        "                                        right_on='num_type',\n",
        "                                        how='left')\n",
        "\n",
        "dataSet_real_rl.drop(columns=['num_type'], inplace=True)\n",
        "\n",
        "print(dataSet_real_rl)\n",
        "dataSet_real_rl.head();\n",
        "#dataSet_real_rl.to_csv('resultados_predicciones_rl.csv', index=False)\n",
        "#files.download('resultados_predicciones_rl.csv')\n",
        "\n",
        "print(\"El DataFrame ha sido guardado como 'resultados_predicciones.csv'.\")\n"
      ],
      "metadata": {
        "id": "xsLRDhUC7v5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Si3vGSqdaOLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: de este datasr df_new_urls.to_csv gradico predicciones con edificios\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df_new_urls is your DataFrame and it has columns 'EDIFICIO' and 'predicciones'\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='EDIFICIO', hue='predicciones', data=dataSet_real_rl)\n",
        "plt.title('Predicciones por Edificio')\n",
        "plt.xlabel('Edificio')\n",
        "plt.ylabel('Cantidad de Predicciones')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "06TzIyRvBFRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df_new_urls is your DataFrame and it has columns 'EDIFICIO' and 'predicciones'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='jornada', hue='type', y = 'probabilidad', data=dataSet_real_rl, errorbar=None)\n",
        "\n",
        "plt.ylim(0, 100)  # Eje y desde 0 hasta 100%\n",
        "plt.xlim(-0.5, len(dataSet_real_rl['jornada'].unique()) - 0.5)  # Eje x ajustado según los valores\n",
        "\n",
        "plt.title('Predicciones por jornada')\n",
        "plt.xlabel('Jornada')\n",
        "plt.ylabel('Porcentaje (%)')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TYKWBeB-aO7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloque de código para algoritmo Random Forest"
      ],
      "metadata": {
        "id": "TWxqAC_F_JAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "#dataSet_politicas_utpl.head()\n",
        "\n",
        "X_rf = dataSet_politicas_utpl['url']\n",
        "y_rf = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorizar los datos de texto usando TfidfVectorizer\n",
        "vectorizer_rf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3), stop_words='english')\n",
        "X_train_vect_rf = vectorizer_rf.fit_transform(X_train_rf)\n",
        "X_test_vect_rf = vectorizer_rf.transform(X_test_rf)\n",
        "\n",
        "# Entrenar el modelo de Random Forest\n",
        "modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "modelo_rf.fit(X_train_vect_rf, y_train_rf)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_rf = modelo_rf.predict(X_test_vect_rf)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\n",
        "print(f'Precisión: {accuracy_rf:.2f}')\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(classification_report(y_test_rf, y_pred_rf))\n",
        "\n",
        "# Mostrar la matriz de confusión\n",
        "print(confusion_matrix(y_test_rf, y_pred_rf))\n",
        "\n",
        "# Guardar el modelo y el vectorizador\n",
        "with open('modelo_random_forest.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo_rf, f)\n",
        "\n",
        "with open('vectorizer_rf.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer_rf, f)"
      ],
      "metadata": {
        "id": "_mqIRb_c_IQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba de modelo random forest implementado"
      ],
      "metadata": {
        "id": "xKN1fkIdr18A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el modelo y el vectorizador\n",
        "with open('modelo_random_forest.pkl', 'rb') as f:\n",
        "    modelo_test_rf = pickle.load(f)\n",
        "\n",
        "with open('vectorizer_rf.pkl', 'rb') as f:\n",
        "    vectorizer_rf = pickle.load(f)\n",
        "\n",
        "# 2. Definir URLs ficticias\n",
        "data_url = {\n",
        "    'url': ['http://youtube.com', 'http://netflix.com']\n",
        "}\n",
        "dataSet_test_rf = pd.DataFrame(data_url)\n",
        "dataSet_test_rf['url'] = dataSet_test_rf['url'].apply(normalizar_url)\n",
        "\n",
        "# 3. Vectorizar las URLs\n",
        "X_test_rf = vectorizer_rf.transform(dataSet_test_rf['url'])\n",
        "\n",
        "y_pred_rf = modelo_test_rf.predict(X_test_rf)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades_rf = modelo_test_rf.predict_proba(X_test_rf)\n",
        "\n",
        "print(\"Predicciones:\", y_pred_rf)\n",
        "print(\"Forma de probabilidades:\", probabilidades_rf.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_test_rf['probabilidad'] = [probabilidades[i, y_pred_rf[i]]\n",
        "                               for i in range(len(y_pred_rf))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_test_rf['predicciones'] = y_pred_rf\n",
        "print(dataSet_test_rf)"
      ],
      "metadata": {
        "id": "gdrKidKSr1K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba con datos reales"
      ],
      "metadata": {
        "id": "lNBClOL4vdoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Cargar el modelo y el vectorizador\n",
        "with open('modelo_random_forest.pkl', 'rb') as f:\n",
        "    modelo_real_rf = pickle.load(f)\n",
        "\n",
        "with open('vectorizer_rf.pkl', 'rb') as f:\n",
        "    vectorizer_rf = pickle.load(f)\n",
        "\n",
        "# 2. Definir URLs ficticias\n",
        "dataSet_real_rf = dataSet_web_utpl.copy()\n",
        "dataSet_real_rf['url'] = dataSet_real_rf['url'].apply(normalizar_url)\n",
        "\n",
        "# 3. Vectorizar las URLs\n",
        "X_real_rf = vectorizer_rf.transform(dataSet_real_rf['url'])\n",
        "\n",
        "y_pred_rf = modelo_real_rf.predict(X_real_rf)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades_rf = modelo_real_rf.predict_proba(X_real_rf)\n",
        "\n",
        "print(\"Predicciones:\", y_pred_rf)\n",
        "print(\"Forma de probabilidades:\", probabilidades_rf.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_real_rf['probabilidad'] = [probabilidades[i, y_pred_rf[i]]\n",
        "                               for i in range(len(y_pred_rf))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_real_rf['predicciones'] = y_pred_rf\n",
        "print(dataSet_real_rf)\n",
        "\n",
        "dataSet_real_rf.to_csv('resultados_predicciones_rf.csv', index=False)\n",
        "#files.download('resultados_predicciones_rf.csv')\n",
        "\n",
        "print(\"El DataFrame ha sido guardado como 'resultados_predicciones_rf.csv'.\")"
      ],
      "metadata": {
        "id": "8ZKmf20ivg2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "estadisticas"
      ],
      "metadata": {
        "id": "bUPn-XcTa3ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "16N7e6kcCRp_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltSX763kCSa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Mapeo de categorías\n",
        "category_mapping = {\n",
        "    0: \"Business and Economy\",\n",
        "    1: \"Games\",\n",
        "    2: \"Phishing\",\n",
        "    3: \"Shopping\",\n",
        "    4: \"Streaming Media\"\n",
        "}\n",
        "\n",
        "# Convertir valores numéricos a nombres de categorías\n",
        "dataSet_real_rl['categoria'] = dataSet_real_rl['predicciones'].map(category_mapping)\n",
        "\n",
        "# Contar el número de predicciones por jornada y categoría\n",
        "count_data = dataSet_real_rl.groupby(['jornada', 'categoria']).size().reset_index(name='cantidad')\n",
        "\n",
        "# Convertir los conteos en porcentaje por jornada\n",
        "total_por_jornada = count_data.groupby('jornada')['cantidad'].transform('sum')\n",
        "count_data['porcentaje'] = (count_data['cantidad'] / total_por_jornada) * 100\n",
        "\n",
        "# Graficar en porcentaje\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x='jornada', y='porcentaje', hue='categoria', data=count_data)\n",
        "\n",
        "# Agregar etiquetas de porcentaje a cada barra\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height > 0:  # Evitar etiquetas en barras de 0%\n",
        "        ax.annotate(f'{height:.1f}%',\n",
        "                    (p.get_x() + p.get_width() / 2., height),\n",
        "                    ha='center', va='bottom', fontsize=10, color='black', fontweight='bold')\n",
        "\n",
        "# Personalización del gráfico\n",
        "plt.title('Predicciones por Jornada en Porcentaje')\n",
        "plt.xlabel('Jornada')\n",
        "plt.ylabel('Porcentaje de Predicciones')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title=\"Categoría\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WB8u-P3da4Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Mapeo de categorías\n",
        "category_mapping = {\n",
        "    0: \"Business and Economy\",\n",
        "    1: \"Games\",\n",
        "    2: \"Phishing\",\n",
        "    3: \"Shopping\",\n",
        "    4: \"Streaming Media\"\n",
        "}\n",
        "\n",
        "# Agregar la columna de categorías a partir de las predicciones\n",
        "dataSet_real_rl['category'] = dataSet_real_rl['predicciones'].map(category_mapping)\n",
        "\n",
        "# Graficar las predicciones por edificio con categorías\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='EDIFICIO', hue='category', data=dataSet_real_rl)\n",
        "\n",
        "# Personalización del gráfico\n",
        "plt.title('Predicciones por Edificio')\n",
        "plt.xlabel('Edificio')\n",
        "plt.ylabel('Cantidad de Predicciones')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotar etiquetas del eje X para mejor lectura\n",
        "plt.legend(title=\"Categoría\")  # Agregar leyenda con el título adecuado\n",
        "plt.tight_layout()  # Ajustar diseño para evitar superposición\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UJmY4axgM_2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "72Q9m3FSOdS8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4e0pg6smQ097"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTACIÓN ALGORITMO SVM"
      ],
      "metadata": {
        "id": "K_IM-E8G41Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cISxMZFFL-JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# 2. Definir las etiquetas\n",
        "X_svm = dataSet_politicas_utpl['url']\n",
        "y_svm = dataSet_politicas_utpl['num_type']\n",
        "\n",
        "# 3. Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm, y_svm, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Vectorizar los datos\n",
        "vectorizer_svm = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "X_train_vectorizado_svm = vectorizer_svm.fit_transform(X_train_svm)\n",
        "X_test_vectorizado_svm = vectorizer_svm.transform(X_test_svm)\n",
        "\n",
        "# 5. Definir el modelo SVM sin ajuste de hiperparámetros\n",
        "modelo_svm = SVC(probability=True, random_state=42)  # Puedes cambiar probability a False si no necesitas probabilidades\n",
        "\n",
        "# 6. Entrenar el modelo\n",
        "modelo_svm.fit(X_train_vectorizado_svm, y_train_svm)\n",
        "\n",
        "# 7. Hacer predicciones\n",
        "predicciones_svm = modelo_svm.predict(X_test_vectorizado_svm)\n",
        "\n",
        "# 8. Mostrar resultados\n",
        "print(\"Precisión (SVM):\", accuracy_score(y_test_svm, predicciones_svm))\n",
        "print(classification_report(y_test_svm, predicciones_svm))\n",
        "\n",
        "# 9. Guardar el modelo y el vectorizador\n",
        "with open('modelo_svm.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo_svm, f)\n",
        "\n",
        "with open('vectorizer_svm.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer_svm, f)\n",
        "\n",
        "print(\"Modelo y vectorizador guardados.\")"
      ],
      "metadata": {
        "id": "Yg85wgtwBA3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruebas con modelo SVM"
      ],
      "metadata": {
        "id": "ZOXUCn017czy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el modelo y el vectorizador\n",
        "with open('modelo_svm.pkl', 'rb') as f:\n",
        "    modelo_test_svm = pickle.load(f)\n",
        "\n",
        "with open('vectorizer_svm.pkl', 'rb') as f:\n",
        "    vectorizer_svm = pickle.load(f)\n",
        "\n",
        "# 2. Definir URLs ficticias\n",
        "data_url = {\n",
        "    'url': ['http://youtube.com', 'http://netflix.com']\n",
        "}\n",
        "dataSet_test_svm = pd.DataFrame(data_url)\n",
        "dataSet_test_svm['url'] = dataSet_test_svm['url'].apply(normalizar_url)\n",
        "\n",
        "# 3. Vectorizar las URLs\n",
        "X_test_svm = vectorizer_svm.transform(dataSet_test_svm['url'])\n",
        "\n",
        "y_pred_svm = modelo_test_svm.predict(X_test_svm)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades_svm = modelo_test_svm.predict_proba(X_test_svm)\n",
        "\n",
        "print(\"Predicciones:\", y_pred_svm)\n",
        "print(\"Forma de probabilidades:\", probabilidades_svm.shape)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_test_svm['probabilidad'] = [probabilidades[i, y_pred_svm[i]]\n",
        "                               for i in range(len(y_pred_svm))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_test_svm['predicciones'] = y_pred_svm\n",
        "print(dataSet_test_svm)"
      ],
      "metadata": {
        "id": "Ihgq7kX87huH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba con datos reales SVM"
      ],
      "metadata": {
        "id": "TEMWEzpN87nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Cargar el modelo de vectorización\n",
        "with open('vectorizer_svm.pkl', 'rb') as f:\n",
        "    vectorizer_svm = pickle.load(f)\n",
        "\n",
        "# 2. Cargar el modelo de regresión logística\n",
        "with open('modelo_svm.pkl', 'rb') as f:\n",
        "    modelo_real_svm = pickle.load(f)\n",
        "\n",
        "# 3. Cargar el nuevo dataset que solo contiene la columna URL\n",
        "dataSet_real_svm = dataSet_web_utpl.copy()\n",
        "dataSet_real_svm['url'] = dataSet_real_svm['url'].apply(normalizar_url)\n",
        "\n",
        "# 4. Vectorizar las nuevas URLs\n",
        "X_real_svm = vectorizer_svm.transform(dataSet_real_svm['url'])\n",
        "\n",
        "# 5. Realizar predicciones\n",
        "y_new_svm = modelo_real_svm.predict(X_real_svm)\n",
        "\n",
        "# 6. Obtener las probabilidades de predicción\n",
        "probabilidades_svm = modelo_real_svm.predict_proba(X_real_svm)\n",
        "\n",
        "# 7. Extraer la probabilidad de la clase predicha\n",
        "# Suponiendo que la clase predicha es la primera en el array de probabilidades\n",
        "dataSet_real_svm['probabilidad'] = [probabilidades[i, y_new_svm[i]]\n",
        "                               for i in range(len(y_new_svm))]\n",
        "\n",
        "# 8. (Opcional) Mostrar las predicciones con sus URLs\n",
        "dataSet_real_svm['predicciones'] = y_new_svm\n",
        "print(dataSet_real_svm)\n",
        "\n",
        "dataSet_real_svm.head();\n",
        "dataSet_real_svm.to_csv('resultados_predicciones_svm.csv', index=False)\n",
        "#files.download('resultados_predicciones_svm.csv')\n",
        "\n",
        "print(\"El DataFrame ha sido guardado como 'resultados_predicciones_svm.csv'.\")\n"
      ],
      "metadata": {
        "id": "TaOfsUxZ8_Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importaciones"
      ],
      "metadata": {
        "id": "Wkuh8iO9aVt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV"
      ],
      "metadata": {
        "id": "-ZtFhc9jadO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparación de Datos"
      ],
      "metadata": {
        "id": "sIkrsnCyVjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preparar_datos(data):\n",
        "    \"\"\"Preparar características y variable objetivo.\"\"\"\n",
        "    X = data['url']\n",
        "    y = data['num_type']\n",
        "    return train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "X-pygYPyVd49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizar datos"
      ],
      "metadata": {
        "id": "J1p3FLm-WPvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizar_datos(X_train, X_test):\n",
        "    \"\"\"Vectorizar datos usando TfidfVectorizer.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "    X_train_vectorizado = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorizado = vectorizer.transform(X_test)\n",
        "    return X_train_vectorizado, X_test_vectorizado, vectorizer"
      ],
      "metadata": {
        "id": "6pMZO-2tWVYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluar Modelo"
      ],
      "metadata": {
        "id": "v4MKzyvZWfFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluar_modelo(modelo, X_test, y_test):\n",
        "    \"\"\"Evaluar el modelo y mostrar resultados.\"\"\"\n",
        "    prediccion_general = modelo.predict(X_test)\n",
        "    precision_general = accuracy_score(y_test, prediccion_general)\n",
        "    f1_val = f1_score(y_test, prediccion_general, average='weighted')\n",
        "    print(f'Precisión: {precision_general:.2f}')\n",
        "    # Mostrar el reporte de clasificación\n",
        "    print(classification_report(y_test, prediccion_general))\n",
        "\n",
        "    # Mostrar la matriz de confusión\n",
        "    print(confusion_matrix(y_test, prediccion_general))\n",
        "    return prediccion_general, precision_general, f1_val"
      ],
      "metadata": {
        "id": "Jx2WZxTTWmgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos de entrenamiento GridSearchCV y RandomizedSearchCV"
      ],
      "metadata": {
        "id": "rBeZOvSxUbw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entrenar_GridSearchCV(modelo, param_grid, X_train, y_train):\n",
        "    \"\"\"Entrenar el modelo usando GridSearchCV y devolver el mejor modelo.\"\"\"\n",
        "    grid_search = GridSearchCV(estimator=modelo, param_grid=param_grid,\n",
        "                               cv=3, scoring='f1_weighted')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_, grid_search.best_params_\n",
        "\n",
        "def entrenar_RandomizedSearchCV(modelo, param_grid, X_train, y_train, n_iter=100):\n",
        "    \"\"\"Entrenar el modelo usando RandomizedSearchCV y devolver el mejor modelo.\"\"\"\n",
        "    random_search = RandomizedSearchCV(estimator=modelo, param_distributions=param_grid,\n",
        "                                       n_iter=n_iter, cv=5, n_jobs=-1, verbose=2, scoring='accuracy', random_state=42)\n",
        "    random_search.fit(X_train, y_train)\n",
        "    return random_search.best_estimator_, random_search.best_params_"
      ],
      "metadata": {
        "id": "Q41hsrzIUFPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dibujar Matriz de confusión"
      ],
      "metadata": {
        "id": "-8PIEXuBU4Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dibujar_matriz_confusion(y_test, predic, nombre):\n",
        "    \"\"\"Dibujar la matriz de confusión.\"\"\"\n",
        "    conf_matrix = confusion_matrix(y_test, predic)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Matriz de Confusión - {nombre}\")\n",
        "    plt.xlabel(\"Predicción\")\n",
        "    plt.ylabel(\"Realidad\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "amRlWrw4U74k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparación de datos"
      ],
      "metadata": {
        "id": "zhu9BARnXvzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = preparar_datos(dataSet_politicas_utpl)\n",
        "\n",
        "\"\"\"Vectorización de datos\"\"\"\n",
        "X_train_vectorizado, X_test_vectorizado, vectorizer = vectorizar_datos(X_train, X_test)\n",
        "\n",
        "with open('vectorizer.pkl', 'wb') as f:\n",
        "     pickle.dump(vectorizer, f)\n",
        "print(\"Modelo de vectorización guardado exitosamente.\")\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],  # Ejemplo de aumento del rango\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "param_grid_lr = {\n",
        "    \"C\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "param_grid_svc = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "param_grid_grad = {\n",
        "\"learning_rate\": [0.01, 0.1, 0.2],\n",
        "\"n_estimators\": [50, 100, 200],\n",
        "\"max_depth\": [3, 5, 10]\n",
        "}\n",
        "\n",
        "\n",
        "resultados = {}\n",
        "mejores_hiperparametros = {}"
      ],
      "metadata": {
        "id": "M1VimU-EXtPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ramdom Forest"
      ],
      "metadata": {
        "id": "-MhyU4UQaWOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"RANDOM FOREST\"\"\"\n",
        "modelo, params = entrenar_GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid_rf,\n",
        "    X_train_vectorizado,\n",
        "    y_train\n",
        "    )\n",
        "mejores_hiperparametros['Random Forest'] = params\n",
        "predic, presicion, f1_val  = evaluar_modelo(modelo, X_test_vectorizado, y_test)\n",
        "resultados['Random Forest'] = {\"Accuracy\": presicion, \"F1-Score\": f1_val}\n",
        "with open('modelo_random_forest1.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo, f)\n",
        "print(\"Modelo de Random Forest guardado exitosamente.\")\n",
        "dibujar_matriz_confusion(y_test, predic, 'Random Forest')"
      ],
      "metadata": {
        "id": "QXuw_rXHaVJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresión Logística"
      ],
      "metadata": {
        "id": "_CB1y_Z4agmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"LOGISTIC REGRESSION\"\"\"\n",
        "modelo, params = entrenar_GridSearchCV(\n",
        "    LogisticRegression(max_iter=5000, random_state=42),\n",
        "    param_grid_lr,\n",
        "    X_train_vectorizado,\n",
        "    y_train\n",
        "    )\n",
        "mejores_hiperparametros['Regresión Logística'] = params\n",
        "predic, presicion, f1_val  = evaluar_modelo(modelo, X_test_vectorizado, y_test)\n",
        "resultados['Regresión Logística'] = {\"Accuracy\": presicion, \"F1-Score\": f1_val}\n",
        "with open('modelo_regresion_logistica1.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo, f)\n",
        "\n",
        "print(\"Modelo de regresión logística guardado exitosamente.\")\n",
        "dibujar_matriz_confusion(y_test, predic, 'Regresión Logística')"
      ],
      "metadata": {
        "id": "bwnJ262kajpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "ATD4ZkfianDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"SUPPORT VECTOR MACHINE\"\"\"\n",
        "modelo, params = entrenar_GridSearchCV(\n",
        "    SVC(probability=True, random_state=42),\n",
        "    param_grid_svc,\n",
        "    X_train_vectorizado,\n",
        "    y_train\n",
        "    )\n",
        "mejores_hiperparametros['SVM'] = params\n",
        "predic, presicion, f1_val  = evaluar_modelo(modelo, X_test_vectorizado, y_test)\n",
        "resultados['SVM'] = {\"Accuracy\": presicion, \"F1-Score\": f1_val}\n",
        "with open('modelo_SVM1.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo, f)\n",
        "\n",
        "print(\"Modelo de regresión logística guardado exitosamente.\")\n",
        "dibujar_matriz_confusion(y_test, predic, 'SVM')"
      ],
      "metadata": {
        "id": "UBIqPJHjao_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"GradientBoostingClassifier\"\"\"\n",
        "modelo, params = entrenar_GridSearchCV(\n",
        "    GradientBoostingClassifier(random_state=42),\n",
        "    param_grid_grad,\n",
        "    X_train_vectorizado,\n",
        "    y_train\n",
        "    )\n",
        "mejores_hiperparametros['Gradient Boosting'] = params\n",
        "predic, presicion, f1_val  = evaluar_modelo(modelo, X_test_vectorizado, y_test)\n",
        "resultados['Gradient Boosting'] = {\"Accuracy\": presicion, \"F1-Score\": f1_val}\n",
        "with open('modelo_random_forest1.pkl', 'wb') as f:\n",
        "    pickle.dump(modelo, f)\n",
        "print(\"Modelo de Gradient Boosting guardado exitosamente.\")\n",
        "dibujar_matriz_confusion(y_test, predic, 'Gradient Boosting')"
      ],
      "metadata": {
        "id": "_fw8ktxRJY8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mejores Hiperparametros"
      ],
      "metadata": {
        "id": "j9ngvHNGcBfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mejores Hiperparametros', mejores_hiperparametros)\n",
        "print('resultados', resultados)"
      ],
      "metadata": {
        "id": "j6q2tSYacEC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparación Resultados"
      ],
      "metadata": {
        "id": "FZXB5QHodo79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_resultados = pd.DataFrame(resultados).T\n",
        "dataSet_resultados.sort_values(by=\"F1-Score\", ascending=False, inplace=True)\n",
        "print(\"\\nComparación de Modelos Categóricos:\")\n",
        "print(dataSet_resultados)"
      ],
      "metadata": {
        "id": "kAWrJAxidszI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función para probar url"
      ],
      "metadata": {
        "id": "xMAZj92hsfzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_url(vectorizer, modelo, df):\n",
        "    df['url'] = df['url'].apply(normalizar_url)\n",
        "    url_vectorizada = vectorizer.transform(df['url'])\n",
        "\n",
        "    probabilidad = modelo.predict_proba(url_vectorizada)  # Devuelve las probabilidades\n",
        "    prediccion = modelo.predict(url_vectorizada)  # Devuelve la clase predicha\n",
        "\n",
        "    df['prediccion'] = prediccion\n",
        "    df['probabilidad'] = probabilidad[:, 1] * 100\n",
        "\n",
        "    \"\"\"df['probabilidad'] *= 100\"\"\"\n",
        "    df = df.merge(dataSet_classify_url_types,\n",
        "                  left_on='prediccion',  # Columna de predicción en el DataFrame original\n",
        "                  right_on='num_type',\n",
        "                  how='left')\n",
        "\n",
        "    df.drop(columns=['num_type'], inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "-76hNrLGsrLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos reales de prueba"
      ],
      "metadata": {
        "id": "fLOT6xTQtb2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresión Logística"
      ],
      "metadata": {
        "id": "eJAxw0EK3tzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "print(\"Modelo de vectorización cargado exitosamente.\")\n",
        "\n",
        "with open('modelo_regresion_logistica1.pkl', 'rb') as f:\n",
        "    modelo = pickle.load(f)\n",
        "\n",
        "print(\"Modelo de regresión logística cargado exitosamente.\")\n",
        "\n",
        "dataSet_real= dataSet_web_utpl.copy()\n",
        "\n",
        "df_resultados = predecir_url(vectorizer, modelo, dataSet_real)\n",
        "\n",
        "print(df_resultados[['url', 'prediccion', 'type', 'probabilidad']])\n",
        "df_resultados.head();\n",
        "#df_resultados.to_csv('resultados_predicciones.csv', index=False)\n",
        "#files.download('resultados_predicciones.csv')"
      ],
      "metadata": {
        "id": "naW5Sv0wtfdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "kUqZayop38yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "print(\"Modelo de vectorización cargado exitosamente.\")\n",
        "\n",
        "with open('modelo_random_forest1.pkl', 'rb') as f:\n",
        "    modelo = pickle.load(f)\n",
        "\n",
        "print(\"Modelo de random forest cargado exitosamente.\")\n",
        "\n",
        "dataSet_real= dataSet_web_utpl.copy()\n",
        "\n",
        "df_resultados = predecir_url(vectorizer, modelo, dataSet_real)\n",
        "\n",
        "print(df_resultados[['url', 'prediccion', 'type', 'probabilidad']])\n",
        "df_resultados.head();\n",
        "#df_resultados.to_csv('resultados_predicciones.csv', index=False)\n",
        "#files.download('resultados_predicciones.csv')"
      ],
      "metadata": {
        "id": "fxpmstMJ3_qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "Lrd2P5mf4MNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "print(\"Modelo de vectorización cargado exitosamente.\")\n",
        "\n",
        "with open('modelo_SVM1.pkl', 'rb') as f:\n",
        "    modelo = pickle.load(f)\n",
        "\n",
        "print(\"Modelo de SVM cargado exitosamente.\")\n",
        "\n",
        "dataSet_real= dataSet_web_utpl.copy()\n",
        "\n",
        "df_resultados = predecir_url(vectorizer, modelo, dataSet_real)\n",
        "\n",
        "print(df_resultados[['url', 'prediccion', 'type', 'probabilidad']])\n",
        "df_resultados.head();\n",
        "#df_resultados.to_csv('resultados_predicciones.csv', index=False)\n",
        "#files.download('resultados_predicciones.csv')"
      ],
      "metadata": {
        "id": "8cns5Vqr4OCy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}